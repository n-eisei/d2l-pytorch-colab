{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0435ce3",
      "metadata": {},
      "source": "\nこのノートブックを実行するには、次の追加ライブラリが必要です。 Colab での実行は実験的なものであることに注意してください。問題がある場合は、Github の問題を報告してください。\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886de6d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-beta0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f853a94",
      "metadata": {},
      "source": "\n# 合成回帰データ\n\n:label: `sec_synthetic-regression-data`\n\n機械学習とは、データから情報を抽出することです。それでは、合成データから何が学べるのかと疑問に思うかもしれません。私たち自身が人工データ生成モデルに焼き付けたパターンについては本質的には気にしていないかもしれませんが、それでもそのようなデータセットは教育目的に役立ち、学習アルゴリズムの特性を評価し、実装が期待どおりに機能することを確認するのに役立ちます。たとえば、正しいパラメーターが*事前に*わかっているデータを作成した場合、モデルが実際にそれらを回復できることを検証できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "62d81e18",
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a490aca9",
      "metadata": {},
      "source": "\n## データセットの生成\n\nこの例では、簡潔にするために低次元で作業します。次のコード スニペットは、標準正規分布から抽出された 2 次元特徴を含む 1000 個の例を生成します。結果の計画行列 $\\mathbf{X}$ は $\\mathbb{R}^{1000 \\times 2}$ に属します。*グラウンド トゥルース*線形関数を適用して各ラベルを生成し、加法ノイズ $\\epsilon$ によってラベルを破損し、各例で独立して同一に描画します。\n\n ( **$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$** )\n\n便宜上、$\\epsilon$ は平均 $\\mu= 0$、標準偏差 $\\sigma = 0.01$ の正規分布から抽出されると仮定します。オブジェクト指向設計の場合、 `d2l.DataModule` ( :numref: `oo-design-data`で紹介) のサブクラスの`__init__`メソッドにコードを追加することに注意してください。追加のハイパーパラメータを設定できるようにすることをお勧めします。これは`save_hyperparameters()`で実現します。 `batch_size`後で決定されます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a731edcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SyntheticRegressionData(d2l.DataModule):  #@save\n",
        "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
        "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
        "                 batch_size=32):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        n = num_train + num_val\n",
        "        self.X = torch.randn(n, len(w))\n",
        "        noise = torch.randn(n, 1) * noise\n",
        "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45db9780",
      "metadata": {},
      "source": "\n以下では、真のパラメータを $\\mathbf{w} = [2, -3.4]^\\top$ および $b = 4.2$ に設定します。後で、推定されたパラメーターをこれらの*グラウンド トゥルース*値と照合して確認できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4b4f853f",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b6cd42",
      "metadata": {},
      "source": "\n[ **`features`の各行は $\\mathbb{R}^2$ のベクトルで構成され、 `labels`の各行はスカラーです。** 】 最初のエントリーを見てみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "44a92bc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features: tensor([-0.0499, -0.2817]) \n",
            "label: tensor([5.0533])\n"
          ]
        }
      ],
      "source": [
        "print('features:', data.X[0],'\\nlabel:', data.y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b127139",
      "metadata": {},
      "source": "\n## データセットの読み取り\n\n機械学習モデルのトレーニングには、多くの場合、データセット上で複数のパスを実行し、一度に 1 つのサンプルのミニバッチを取得する必要があります。このデータはモデルの更新に使用されます。これがどのように機能するかを説明するために、[ **`get_dataloader`メソッドを実装し]、** `add_to_class` (:numref: `oo-design-utilities`で導入) を介してそれを`SyntheticRegressionData`クラスに登録します。これは (**バッチ サイズ、特徴の行列、およびラベルのベクトルを受け取り、サイズ`batch_size`のミニバッチを生成します。** )そのため、各ミニバッチは特徴とラベルのタプルで構成されます。トレーニング モードと検証モードのどちらにいるかに注意する必要があることに注意してください。前者ではデータをランダムな順序で読み取る必要がありますが、後者の場合は、事前に定義された順序でデータを読み取ることができるため、デバッグの目的で重要です。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5b0af353",
      "metadata": {},
      "outputs": [],
      "source": [
        "@d2l.add_to_class(SyntheticRegressionData)\n",
        "def get_dataloader(self, train):\n",
        "    if train:\n",
        "        indices = list(range(0, self.num_train))\n",
        "        # The examples are read in random order\n",
        "        random.shuffle(indices)\n",
        "    else:\n",
        "        indices = list(range(self.num_train, self.num_train+self.num_val))\n",
        "    for i in range(0, len(indices), self.batch_size):\n",
        "        batch_indices = torch.tensor(indices[i: i+self.batch_size])\n",
        "        yield self.X[batch_indices], self.y[batch_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f62647b4",
      "metadata": {},
      "source": "\n直感を構築するために、データの最初のミニバッチを調べてみましょう。特徴の各ミニバッチは、そのサイズと入力特徴の次元の両方を提供します。同様に、ラベルのミニバッチは、 `batch_size`で指定された一致する形状になります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "35d22815",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: torch.Size([32, 2]) \n",
            "y shape: torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "X, y = next(iter(data.train_dataloader()))\n",
        "print('X shape:', X.shape, '\\ny shape:', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d05b9a5",
      "metadata": {},
      "source": "\n一見無害に見えますが、 `iter(data.train_dataloader())`の呼び出しは、Python のオブジェクト指向設計の威力を示しています。 `data`オブジェクトの作成*後に、* `SyntheticRegressionData`クラスにメソッドを追加したことに注意してください。それにもかかわらず、オブジェクトはクラスへの機能の*事後*追加から恩恵を受けます。\n\n反復を通じて、データセット全体が使い果たされるまで、個別のミニバッチを取得します (これを試してください)。上記で実装された反復は教訓的な目的には適していますが、実際の問題で問題が発生する可能性がある点で非効率的です。たとえば、すべてのデータをメモリにロードし、大量のランダム メモリ アクセスを実行する必要があります。深層学習フレームワークに実装された組み込みイテレータはかなり効率的で、ファイルに保存されたデータ、ストリーム経由で受信したデータ、オンザフライで生成または処理されたデータなどのソースを処理できます。次に、組み込みイテレータを使用して同じメソッドを実装してみましょう。\n\n## データローダーの簡潔な実装\n\n独自のイテレータを作成する代わりに、[**フレームワーク内の既存の API を呼び出してデータをロードできます。** ] 前と同様に、特徴`X`とラベル`y`を持つデータセットが必要です。さらに、組み込みデータローダーで`batch_size`を設定し、サンプルのシャッフルを効率的に処理させます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a3eab3a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.DataModule)  #@save\n",
        "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
        "    tensors = tuple(a[indices] for a in tensors)\n",
        "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
        "    return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
        "                                       shuffle=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a16f8bb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
        "def get_dataloader(self, train):\n",
        "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
        "    return self.get_tensorloader((self.X, self.y), train, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3f0faa",
      "metadata": {},
      "source": "\n新しいデータ ローダーは、より効率的でいくつかの機能が追加されている点を除いて、以前のデータ ローダーと同じように動作します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ab4e60ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: torch.Size([32, 2]) \n",
            "y shape: torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "X, y = next(iter(data.train_dataloader()))\n",
        "print('X shape:', X.shape, '\\ny shape:', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a0deab",
      "metadata": {},
      "source": "\nたとえば、フレームワーク API によって提供されるデータ ローダーは組み込みの`__len__`メソッドをサポートしているため、その長さ、つまりバッチの数をクエリできます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0c219edf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data.train_dataloader())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6a82cc",
      "metadata": {},
      "source": "\n## まとめ\n\nデータ ローダーは、データのロードと操作のプロセスを抽象化する便利な方法です。このようにして、同じ機械学習*アルゴリズムで*、変更を加えることなく、さまざまな種類やソースのデータを処理できます。データ ローダーの優れた点の 1 つは、データ ローダーを構成できることです。たとえば、画像をロードし、それをトリミングしたり変更したりする後処理フィルターを使用する場合があります。したがって、データ ローダーを使用して、データ処理パイプライン全体を記述することができます。\n\nモデル自体に関しては、2 次元線形モデルは、私たちが遭遇する可能性があるのと同じくらい単純なモデルです。これにより、データ量が不十分であることや方程式系が不完全に決定されていることを心配することなく、回帰モデルの精度をテストできます。これを次のセクションで有効に活用します。\n\n## 演習\n1. サンプルの数をバッチ サイズで割ることができない場合はどうなりますか。フレームワークの API を使用して別の引数を指定してこの動作を変更するにはどうすればよいですか?\n1. パラメーター ベクトル`w`のサイズとサンプル数`num_examples`の両方が大きい、巨大なデータセットを生成したい場合はどうすればよいでしょうか?<ol><li>すべてのデータをメモリに保持できない場合はどうなるのでしょうか?\n1. データがディスク上に保持されている場合、データをどのようにシャッフルしますか?あなたのタスクは、ランダムな読み取りまたは書き込みをあまり必要としない*効率的な*アルゴリズムを設計することです。ヒント:[擬似ランダム順列ジェネレーターを使用すると、](https://en.wikipedia.org/wiki/Pseudorandom_permutation)順列テーブルを明示的に保存しなくても再シャッフルを設計できます (引用: `Naor.Reingold.1999` )。\n"
    },
    {
      "cell_type": "markdown",
      "id": "210e1d8a",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/6663)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}