{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29dfd780",
      "metadata": {},
      "source": "\n# ガウス過程\n\n:ラベル: `chap_gp`\n\n**アンドリュー・ゴードン・ウィルソン**(*ニューヨーク大学およびアマゾン*)\n\nガウス過程 (GP) は遍在しています。あなたは、気づかないうちにすでに多くの GP の例に遭遇しているでしょう。パラメータが線形で、パラメータ全体にわたってガウス分布を持つモデルはすべてガウス過程です。このクラスは、ランダム ウォークや自己回帰プロセスを含む離散モデルだけでなく、ベイジアン線形回帰モデル、多項式、フーリエ級数、動径基底関数、さらには無限の数の隠れユニットを持つニューラル ネットワークを含む連続モデルにも及びます。 「すべてはガウス過程の特殊なケースである」というジョークが流行しています。\n\nガウス プロセスについて学ぶことは、次の 3 つの理由から重要です。(1) ガウス プロセスは、モデリングの*関数空間*の視点を提供し、ディープ ニューラル ネットワークを含むさまざまなモデル クラスを理解しやすくします。 (2) アクティブ ラーニング、ハイパーパラメータ学習、自動 ML、時空間回帰など、最先端の応用範囲が非常に広いです。 (3) 過去数年間のアルゴリズムの進歩により、ガウス プロセスはますます拡張性と関連性が増し、 [GPyTorch](https://gpytorch.ai)などのフレームワークを通じて深層学習と調和しています:cite: `Gardner.Pleiss.Weinberger.Bindel.Wilson.2018` 。実際、GP とディープ ニューラル ネットワークは競合するアプローチではなく、高度に補完的であり、組み合わせることで大きな効果を得ることができます。これらのアルゴリズムの進歩は、ガウス プロセスに関連するだけでなく、深層学習で広く役立つ数値手法の基礎を提供します。\n\nこの章では、ガウス過程を紹介します。入門ノートでは、ガウス プロセスとは何か、またガウス プロセスが関数をどのように直接モデル化するかについて直感的に推論することから始めます。事前分布ノートでは、ガウス過程の事前分布を指定する方法に焦点を当てます。従来の重み空間アプローチを関数空間のモデリングに直接接続します。これは、ディープ ニューラル ネットワークを含む機械学習モデルの構築と理解について推論するのに役立ちます。次に、ガウス過程の一般化特性を制御する、*カーネル*としても知られる一般的な共分散関数を導入します。特定のカーネルを備えた GP は、事前関数を定義します。推論ノートブックでは、予測を行うためにデータを使用して*事後を*推論する方法を示します。このノートブックには、ガウス プロセスで予測を行うための最初からのコードと、GPyTorch の概要が含まれています。今後のノートブックでは、ガウス プロセスの背後にある数値を紹介します。これは、ガウス プロセスのスケーリングに役立つだけでなく、ディープ ラーニングの強力な一般基盤でもあり、ディープ ラーニングでのハイパーパラメーター調整などの高度なユースケースも提供します。この例では、ガウス プロセスを拡張し、ディープ ラーニング機能および PyTorch と緊密に統合されている GPyTorch を使用します。\n\n :begin_tab:toc\n-  [GP-イントロ](gp-intro.ipynb)\n- [GP プライオリティ](gp-priors.ipynb)\n- [GP 推論](gp-inference.ipynb):end_tab:\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}