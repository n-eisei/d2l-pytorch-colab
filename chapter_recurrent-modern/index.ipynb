{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b7ffb8b3",
      "metadata": {},
      "source": "\n# 最新のリカレント ニューラル ネットワーク\n\n:ラベル: `chap_modern_rnn`\n\n前の章では、リカレント ニューラル ネットワーク (RNN) の背後にある重要なアイデアを紹介しました。ただし、畳み込みニューラル ネットワークと同様に、RNN アーキテクチャでも膨大な量の革新が行われ、実際に成功することが証明されたいくつかの複雑な設計が完成しました。特に、最も一般的な設計は、勾配の消失や爆発に代表される、RNN が直面する悪名高い数値不安定性を軽減するメカニズムを備えています。 :numref: `chap_rnn`で、鈍いグラデーション クリッピング ヒューリスティックを適用することで爆発するグラデーションを扱ったことを思い出してください。このハックの有効性にもかかわらず、勾配が消えるという問題は未解決のままです。\n\nこの章では、1997 年に出版された 2 つの論文に由来する、シーケンス用の最も成功した RNN アーキテクチャの背後にある重要なアイデアを紹介します。最初の論文、 *Long Short-Term Memory* :cite: `Hochreiter.Schmidhuber.1997`では、*メモリ セル*が紹介されています。ネットワークの隠れ層にある従来のノードを置き換える計算単位。これらのメモリ セルを使用すると、ネットワークは、以前のリカレント ネットワークが遭遇したトレーニングでの困難を克服できます。直感的には、メモリ セルは、多くの連続するタイム ステップにわたって重み 1 の再発エッジに沿ってカスケードする各メモリ セルの内部状態の値を維持することにより、勾配消失の問題を回避します。一連の乗算ゲートは、メモリ状態へのどの入力を許可するか、およびメモリ状態の内容がモデルの出力に影響を与えるタイミングの両方をネットワークが決定するのに役立ちます。\n\n 2 番目の論文、*双方向リカレント ニューラル ネットワーク*:cite: `Schuster.Paliwal.1997`では、未来 (後続のタイム ステップ) と過去 (先行するタイム ステップ) の両方からの情報を使用して、任意の時点での出力を決定するアーキテクチャが紹介されています。シーケンス。これは、過去の入力のみが出力に影響を与えることができる以前のネットワークとは対照的です。双方向 RNN は、無数のタスクの中でも、自然言語処理におけるシーケンスのラベル付けタスクの主流となっています。幸いなことに、この 2 つの技術革新は相互に排他的ではなく、音素分類 :cite: `Graves.Schmidhuber.2005`と手書き認識 :cite: `graves2008novel`にうまく組み合わされています。\n\nこの章の最初のセクションでは、ゲート付きリカレント ユニット (GRU) と呼ばれる軽量バージョンである LSTM アーキテクチャ、双方向 RNN の背後にある主要なアイデア、および RNN レイヤーがどのようにスタックされて深い RNN を形成するかについて簡単に説明します。続いて、*エンコーダ/デコーダ*アーキテクチャや*ビーム探索*などの重要なアイデアとともに機械翻訳を紹介しながら、シーケンス間のタスクにおける RNN のアプリケーションを検討します。\n\n :begin_tab:toc\n-  [lstm](lstm.ipynb)\n- [グルー](gru.ipynb)\n- [ディープン](deep-rnn.ipynb)\n- [バイーン](bi-rnn.ipynb)\n- [機械翻訳とデータセット](machine-translation-and-dataset.ipynb)\n- [エンコーダ-デコーダ](encoder-decoder.ipynb)\n- [シーケンス2シーケンス](seq2seq.ipynb)\n- [ビーム検索](beam-search.ipynb):end_tab:\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}