{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f27e42c6",
      "metadata": {},
      "source": "\n# 自然言語処理: 事前トレーニング\n\n:label: `chap_nlp_pretrain`\n\n人間にはコミュニケーションが必要です。人間の基本的な必要性から、膨大な量の文書が毎日生成されています。ソーシャル メディア、チャット アプリ、電子メール、製品レビュー、ニュース記事、研究論文、書籍のリッチ テキストを考慮すると、コンピューターがそれらを理解して、人間の言語に基づいて支援を提供したり意思決定を行えるようにすることが重要になります。\n\n*自然言語処理は、*自然言語を使用したコンピューターと人間の間の相互作用を研究します。実際には、 :numref: `sec_language-model`の言語モデルや :numref: `sec_machine_translation`の機械翻訳モデルなどの自然言語処理技術を使用して、テキスト (人間の自然言語) データを処理および分析することが非常に一般的です。\n\nテキストを理解するには、その表現を学ぶことから始めることができます。大規模なコーパスからの既存のテキスト シーケンスを活用する*自己教師あり学習は*、周囲のテキストの他の部分を使用してテキストの隠れた部分を予測するなど、テキスト表現の事前学習に広く使用されています。このようにして、モデルは、*高価な*ラベル付けの労力を必要とせずに、*大量の*テキスト データから監視を通じて学習します。\n\nこの章で説明するように、各単語またはサブワードを個別のトークンとして扱う場合、各トークンの表現は、word2vec、GloVe、または大規模なコーパスのサブワード埋め込みモデルを使用して事前学習できます。事前トレーニング後、各トークンの表現はベクトルになる可能性がありますが、コンテキストが何であっても同じままです。たとえば、「銀行」のベクトル表現は、「お金を預けるために銀行に行く」と「座るために銀行に行く」の両方で同じです。したがって、より最近の多くの事前トレーニング モデルは、同じトークンの表現を異なるコンテキストに適応させています。その中には、Transformer エンコーダに基づいた、より深い自己教師ありモデルである BERT があります。この章では、 :numref: `fig_nlp-map-pretrain`で強調表示されているように、テキストのこのような表現を事前トレーニングする方法に焦点を当てます。 \n\n![](../img/nlp-map-pretrain.svg) :label: `fig_nlp-map-pretrain`\n\n全体像を把握するために、:numref: `fig_nlp-map-pretrain`事前トレーニングされたテキスト表現を、さまざまな下流の自然言語処理アプリケーションのさまざまな深層学習アーキテクチャに供給できることを示しています。これらについては :numref: `chap_nlp_app`で説明します。\n\n :begin_tab:toc\n-  [word2vec](word2vec.ipynb)\n- [おおよそのトレーニング](approx-training.ipynb)\n- [単語埋め込みデータセット](word-embedding-dataset.ipynb)\n- [word2vec-事前トレーニング](word2vec-pretraining.ipynb)\n- [グローブ](glove.ipynb)\n- [サブワード埋め込み](subword-embedding.ipynb)\n- [類似性 - 類似性](similarity-analogy.ipynb)\n- [バート](bert.ipynb)\n- [ベルトデータセット](bert-dataset.ipynb)\n- [bert-pretraining](bert-pretraining.ipynb) :end_tab:\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}