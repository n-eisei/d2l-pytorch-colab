{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd0cc473",
      "metadata": {},
      "source": "\nこのノートブックを実行するには、次の追加ライブラリが必要です。 Colab での実行は実験的なものであることに注意してください。問題がある場合は、Github の問題を報告してください。\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b380846",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-beta0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f86292f5",
      "metadata": {},
      "source": "\n# 単語の類似性と類推\n\n:label: `sec_synonyms`\n\n :numref: `sec_word2vec_pretraining`では、小さなデータセットで word2vec モデルをトレーニングし、それを適用して、入力単語に対して意味的に類似した単語を検索しました。実際には、大規模なコーパスで事前学習された単語ベクトルは、下流の自然言語処理タスクに適用できます。これについては、 :numref: `chap_nlp_app`で後述します。大規模なコーパスからの事前学習済み単語ベクトルのセマンティクスを簡単に示すために、単語の類似性タスクと類推タスクにそれらを適用してみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5de4181b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3399064",
      "metadata": {},
      "source": "\n## 事前学習済みの単語ベクトルの読み込み\n\n以下に、次元 50、100、および 300 の事前トレーニング済み GloVe エンベディングをリストします。これらは、 [GloVe Web サイト](https://nlp.stanford.edu/projects/glove/)からダウンロードできます。事前トレーニングされた fastText 埋め込みは複数の言語で利用できます。ここでは、 [fastText Web サイト](https://fasttext.cc/)からダウンロードできる 1 つの英語版 (300 次元の「wiki.en」) について考えます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c51c0c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
        "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
        "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
        "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
        "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0a09b9",
      "metadata": {},
      "source": "\nこれらの事前トレーニング済みの GloVe および fastText 埋め込みをロードするには、次の`TokenEmbedding`クラスを定義します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "57c1161a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@save\n",
        "class TokenEmbedding:\n",
        "    \"\"\"Token Embedding.\"\"\"\n",
        "    def __init__(self, embedding_name):\n",
        "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
        "            embedding_name)\n",
        "        self.unknown_idx = 0\n",
        "        self.token_to_idx = {token: idx for idx, token in\n",
        "                             enumerate(self.idx_to_token)}\n",
        "\n",
        "    def _load_embedding(self, embedding_name):\n",
        "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
        "        data_dir = d2l.download_extract(embedding_name)\n",
        "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
        "        # fastText website: https://fasttext.cc/\n",
        "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                elems = line.rstrip().split(' ')\n",
        "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
        "                # Skip header information, such as the top row in fastText\n",
        "                if len(elems) > 1:\n",
        "                    idx_to_token.append(token)\n",
        "                    idx_to_vec.append(elems)\n",
        "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
        "        return idx_to_token, torch.tensor(idx_to_vec)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
        "                   for token in tokens]\n",
        "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
        "        return vecs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f18265c",
      "metadata": {},
      "source": "\n以下では、50 次元の GloVe 埋め込み (Wikipedia サブセットで事前トレーニング済み) をロードします。 `TokenEmbedding`インスタンスを作成するとき、指定された埋め込みファイルがまだダウンロードされていない場合はダウンロードする必要があります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d3e4e3ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ../data/glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip...\n"
          ]
        }
      ],
      "source": [
        "glove_6b50d = TokenEmbedding('glove.6b.50d')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f875c17",
      "metadata": {},
      "source": "\n語彙サイズを出力します。語彙には 400,000 の単語 (トークン) と特別な未知のトークンが含まれています。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e42d27a9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c3284c",
      "metadata": {},
      "source": "\n語彙内の単語のインデックスを取得することも、その逆も可能です。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8605156c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3367, 'beautiful')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45fd7267",
      "metadata": {},
      "source": "\n## 事前学習済みの単語ベクトルの適用\n\nロードされた GloVe ベクトルを使用して、次の単語の類似性と類似性のタスクに適用することで、そのセマンティクスを実証します。\n\n### 単語の類似性\n\n:numref: `subsec_apply-word-embed`と同様に、単語ベクトル間のコサイン類似性に基づいて入力単語に対して意味的に類似した単語を見つけるために、次の`knn` ($k$-最近傍) 関数を実装します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "28fd1b11",
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn(W, x, k):\n",
        "    # Add 1e-9 for numerical stability\n",
        "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
        "        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
        "        torch.sqrt((x * x).sum()))\n",
        "    _, topk = torch.topk(cos, k=k)\n",
        "    return topk, [cos[int(i)] for i in topk]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03796dfb",
      "metadata": {},
      "source": "\n次に、 `TokenEmbedding`インスタンス`embed`からの事前学習済みの単語ベクトルを使用して、類似した単語を検索します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9137c391",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
        "    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word\n",
        "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0c88bc",
      "metadata": {},
      "source": "\n`glove_6b50d`の事前トレーニングされた単語ベクトルの語彙には、400,000 の単語と特別な未知のトークンが含まれています。入力単語と未知のトークンを除いて、この語彙の中から「チップ」という単語に意味的に最も似ている単語を 3 つ見つけてみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6f74ba99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cosine sim=0.856: chips\n",
            "cosine sim=0.749: intel\n",
            "cosine sim=0.749: electronics\n"
          ]
        }
      ],
      "source": [
        "get_similar_tokens('chip', 3, glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a53336",
      "metadata": {},
      "source": "\n以下は「baby」と「 beautiful」に似た単語を出力します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1bedf0e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cosine sim=0.839: babies\n",
            "cosine sim=0.800: boy\n",
            "cosine sim=0.792: girl\n"
          ]
        }
      ],
      "source": [
        "get_similar_tokens('baby', 3, glove_6b50d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9bf2d79a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cosine sim=0.921: lovely\n",
            "cosine sim=0.893: gorgeous\n",
            "cosine sim=0.830: wonderful\n"
          ]
        }
      ],
      "source": [
        "get_similar_tokens('beautiful', 3, glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "038c4a32",
      "metadata": {},
      "source": "\n### 言葉のたとえ\n\n類似した単語を見つけるだけでなく、単語ベクトルを単語の類推タスクに適用することもできます。たとえば、「男性」:「女性」::「息子」:「娘」は単語の類似形式です。「男性」は「女性」に対して、「息子」は「娘」に対してです。具体的には、単語類似補完タスクは次のように定義できます。単語類似 $a : b :: c : d$ について、最初の 3 つの単語 $a$、$b$、$c$ が与えられた場合、$d$ を見つけます。単語 $w$ のベクトルを $\\text{vec}(w)$ で表します。類推を完了するには、$\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$ の結果に最も類似するベクトルを持つ単語を見つけます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "feb55537",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_analogy(token_a, token_b, token_c, embed):\n",
        "    vecs = embed[[token_a, token_b, token_c]]\n",
        "    x = vecs[1] - vecs[0] + vecs[2]\n",
        "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
        "    return embed.idx_to_token[int(topk[0])]  # Remove unknown words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b04348c6",
      "metadata": {},
      "source": "\nロードされた単語ベクトルを使用して、「男性と女性」の類似性を検証してみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "977217d5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'daughter'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_analogy('man', 'woman', 'son', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67253639",
      "metadata": {},
      "source": "\n以下は「首都と国」の類似例です: 「北京」:「中国」::「東京」:「日本」。これは、事前トレーニングされた単語ベクトルのセマンティクスを示しています。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ff3ce52f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'japan'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b59a16b",
      "metadata": {},
      "source": "\n「悪い」:「最悪」::「大きな」:「最大の」などの「形容詞-最上級形容詞」のアナロジーでは、事前トレーニングされた単語ベクトルが構文情報をキャプチャしている可能性があることがわかります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc21db73",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'biggest'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_analogy('bad', 'worst', 'big', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6619c92f",
      "metadata": {},
      "source": "\n事前学習済みの単語ベクトルで捕捉された過去時制の概念を示すために、「現在時制 - 過去時制」のアナロジー、つまり「do」:「did」::「go」:「went」を使用して構文をテストできます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f293b2cc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'went'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_analogy('do', 'did', 'go', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e530b439",
      "metadata": {},
      "source": "\n## まとめ\n- 実際には、大規模なコーパスで事前学習された単語ベクトルを下流の自然言語処理タスクに適用できます。\n- 事前トレーニングされた単語ベクトルは、単語の類似性タスクと類推タスクに適用できます。\n\n## 演習\n1. `TokenEmbedding(&#39;wiki.en&#39;)`を使用して fastText の結果をテストします。\n1. 語彙が非常に多い場合、どうすれば似たような単語を見つけたり、単語の類推をより早く完了したりできるでしょうか?\n"
    },
    {
      "cell_type": "markdown",
      "id": "ef303139",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/1336)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}