{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4b8b9af9",
      "metadata": {},
      "source": "\nこのノートブックを実行するには、次の追加ライブラリが必要です。 Colab での実行は実験的なものであることに注意してください。問題がある場合は、Github の問題を報告してください。\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f8da93",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-beta0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c0e8ed",
      "metadata": {},
      "source": "\n# GPU\n\n :label: `sec_use_gpu`\n\n :numref: `tab_intro_decade`では、過去 20 年間にわたる計算の急速な成長について説明しました。一言で言えば、GPU のパフォーマンスは 2000 年以来 10 年ごとに 1000 倍ずつ増加しています。これは大きなチャンスをもたらしますが、同時にそのようなパフォーマンスを提供することが非常に必要であることも示唆しています。\n\nこのセクションでは、この計算パフォーマンスを研究に活用する方法について説明します。最初は単一の GPU を使用し、その後で複数の GPU と複数のサーバー (複数の GPU を使用) を使用する方法について説明します。\n\n具体的には、計算に単一の NVIDIA GPU を使用する方法について説明します。まず、少なくとも 1 つの NVIDIA GPU がインストールされていることを確認してください。次に、 [NVIDIA ドライバーと CUDA](https://developer.nvidia.com/cuda-downloads)をダウンロードし、プロンプトに従って適切なパスを設定します。これらの準備が完了すると、 `nvidia-smi`コマンドを使用して (**グラフィックス カード情報を表示**) できるようになります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "491be4c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Feb 10 06:11:13 2023       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:17.0 Off |                    0 |\r\n",
            "| N/A   35C    P0    76W / 300W |   1534MiB / 16160MiB |     53%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   1  Tesla V100-SXM2...  Off  | 00000000:00:18.0 Off |                    0 |\r\n",
            "| N/A   34C    P0    42W / 300W |      0MiB / 16160MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "|   2  Tesla V100-SXM2...  Off  | 00000000:00:19.0 Off |                    0 |\r\n",
            "| N/A   36C    P0    80W / 300W |   3308MiB / 16160MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   3  Tesla V100-SXM2...  Off  | 00000000:00:1A.0 Off |                    0 |\r\n",
            "| N/A   35C    P0   200W / 300W |   3396MiB / 16160MiB |      4%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "|   4  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |\r\n",
            "| N/A   32C    P0    56W / 300W |   1126MiB / 16160MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   5  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |\r\n",
            "| N/A   40C    P0    84W / 300W |   1522MiB / 16160MiB |     47%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "|   6  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |\r\n",
            "| N/A   34C    P0    57W / 300W |    768MiB / 16160MiB |      3%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "|   7  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
            "| N/A   32C    P0    41W / 300W |      0MiB / 16160MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|    0   N/A  N/A     18049      C   ...l-en-release-1/bin/python     1531MiB |\r\n",
            "|    2   N/A  N/A     41102      C   ...l-en-release-1/bin/python     3305MiB |\r\n",
            "|    3   N/A  N/A     41102      C   ...l-en-release-1/bin/python     3393MiB |\r\n",
            "|    4   N/A  N/A     44560      C   ...l-en-release-1/bin/python     1123MiB |\r\n",
            "|    5   N/A  N/A     18049      C   ...l-en-release-1/bin/python     1519MiB |\r\n",
            "|    6   N/A  N/A     44560      C   ...l-en-release-1/bin/python      771MiB |\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e0c3397",
      "metadata": {},
      "source": "\nPyTorch では、すべての配列にデバイスがあり、それをコンテキストと呼ぶことがよくあります。これまでのところ、デフォルトでは、すべての変数と関連する計算が CPU に割り当てられています。通常、他のコンテキストはさまざまな GPU である可能性があります。複数のサーバーにジョブをデプロイする場合、事態はさらに困難になる可能性があります。配列をコンテキストにインテリジェントに割り当てることで、デバイス間のデータ転送にかかる時間を最小限に抑えることができます。たとえば、GPU を備えたサーバー上でニューラル ネットワークをトレーニングする場合、通常はモデルのパラメーターが GPU 上に存在することを好みます。\n"
    },
    {
      "cell_type": "markdown",
      "id": "e72b490f",
      "metadata": {},
      "source": "\nこのセクションのプログラムを実行するには、少なくとも 2 つの GPU が必要です。これはほとんどのデスクトップ コンピューターにとって贅沢かもしれませんが、AWS EC2 マルチ GPU インスタンスを使用するなど、クラウドで簡単に利用できることに注意してください。他のほとんどすべてのセクションでは複数の GPU は必要あり*ません*。代わりに、これは単に、異なるデバイス間でデータがどのように流れるかを示すためのものです。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a8556ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f0677b",
      "metadata": {},
      "source": "\n## [**コンピューティングデバイス**]\n\nストレージや計算のために CPU や GPU などのデバイスを指定できます。デフォルトでは、テンソルはメイン メモリに作成され、CPU を使用して計算されます。\n"
    },
    {
      "cell_type": "markdown",
      "id": "0725f553",
      "metadata": {},
      "source": "\nPyTorch では、CPU と GPU `torch.device(&#39;cpu&#39;)`と`torch.device(&#39;cuda&#39;)`で指定できます。なお、 `cpu`デバイスとは物理的な CPU やメモリをすべて指します。これは、PyTorch の計算がすべての CPU コアを使用しようとすることを意味します。ただし、 `gpu`デバイスは 1 つのカードと対応するメモリのみを表します。複数の GPU がある場合、 `torch.device(f&#39;cuda:{i}&#39;)`を使用して $i^\\mathrm{th}$ GPU を表します ($i$ は 0 から始まります)。また、 `gpu:0`と`gpu`同等です。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f7d361fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(device(type='cpu'),\n",
              " device(type='cuda', index=0),\n",
              " device(type='cuda', index=1))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cpu():  #@save\n",
        "    \"\"\"Get the CPU device.\"\"\"\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def gpu(i=0):  #@save\n",
        "    \"\"\"Get a GPU device.\"\"\"\n",
        "    return torch.device(f'cuda:{i}')\n",
        "\n",
        "cpu(), gpu(), gpu(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc808b8a",
      "metadata": {},
      "source": "\n(**利用可能な GPU の数をクエリします。** )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c29ed2e9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def num_gpus():  #@save\n",
        "    \"\"\"Get the number of available GPUs.\"\"\"\n",
        "    return torch.cuda.device_count()\n",
        "\n",
        "num_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d68804",
      "metadata": {},
      "source": "\nここで、[**要求された GPU が存在しない場合でもコードを実行できるようにする 2 つの便利な関数を定義します。** 】\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b4d3266",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0),\n",
              " device(type='cpu'),\n",
              " [device(type='cuda', index=0), device(type='cuda', index=1)])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def try_gpu(i=0):  #@save\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if num_gpus() >= i + 1:\n",
        "        return gpu(i)\n",
        "    return cpu()\n",
        "\n",
        "def try_all_gpus():  #@save\n",
        "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
        "    return [gpu(i) for i in range(num_gpus())]\n",
        "\n",
        "try_gpu(), try_gpu(10), try_all_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a153e3c5",
      "metadata": {},
      "source": "\n## テンソルとGPU\n"
    },
    {
      "cell_type": "markdown",
      "id": "a29ddba9",
      "metadata": {},
      "source": "\nデフォルトでは、テンソルは CPU 上に作成されます。 [**テンソルが配置されているデバイスをクエリできます。** 】\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c4d98324",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "x.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3476905c",
      "metadata": {},
      "source": "\n複数の用語を操作したい場合は常に、それらの用語が同じデバイス上にある必要があることに注意することが重要です。たとえば、2 つのテンソルを合計する場合、両方の引数が同じデバイス上に存在することを確認する必要があります。そうしないと、フレームワークは結果をどこに保存するか、あるいは計算を実行する場所を決定する方法さえも認識できなくなります。\n\n###  GPU上のストレージ\n\n[ **GPU にテンソルを保存するには、いくつかの方法があります。** ] たとえば、テンソルを作成するときにストレージ デバイスを指定できます。次に、最初の`gpu`上にテンソル変数`X`を作成します。 GPU で作成されたテンソルは、この GPU のメモリのみを消費します。 `nvidia-smi`コマンドを使用して、GPU メモリの使用状況を表示できます。一般に、GPU メモリ制限を超えるデータを作成しないようにする必要があります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c7a3dfeb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]], device='cuda:0')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.ones(2, 3, device=try_gpu())\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8572cf8",
      "metadata": {},
      "source": "\n少なくとも 2 つの GPU があると仮定すると、次のコードは ( **2 番目の GPU にランダムなテンソルを作成します。** )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e03996a9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7781, 0.1400, 0.4503],\n",
              "        [0.1745, 0.2343, 0.6356]], device='cuda:1')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y = torch.rand(2, 3, device=try_gpu(1))\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1445ab85",
      "metadata": {},
      "source": "\n### コピーする\n\n[ **`X + Y`を計算したい場合は、この操作をどこで実行するかを決定する必要があります。** ] たとえば、 :numref: `fig_copyto`に示すように、 `X` 2 番目の GPU に転送し、そこで操作を実行できます。例外が発生するため、単純に`X`と`Y`を追加し*ないでください*。ランタイム エンジンは何をすべきかわかりません。同じデバイス上でデータが見つからず、失敗します。 `Y` 2 番目の GPU 上に存在するため、2 つを追加する前に`X`そこに移動する必要があります。 \n\n![](http://d2l.ai/_images/copyto.svg) :label: `fig_copyto`\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9accbec5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], device='cuda:0')\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], device='cuda:1')\n"
          ]
        }
      ],
      "source": [
        "Z = X.cuda(1)\n",
        "print(X)\n",
        "print(Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d3e3a20",
      "metadata": {},
      "source": "\n[**データが同じ GPU ( `Z`と`Y`の両方) 上にあるので、それらを合計できます。** 】\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "01b0eeb8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.7781, 1.1400, 1.4503],\n",
              "        [1.1745, 1.2343, 1.6356]], device='cuda:1')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y + Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077c038e",
      "metadata": {},
      "source": "\n変数`Z`がすでに 2 番目の GPU に存在していると想像してください。それでも`Z.cuda(1)`を呼び出すとどうなるでしょうか?コピーを作成して新しいメモリを割り当てる代わりに、 `Z`を返します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fcb923c9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z.cuda(1) is Z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0298ef1",
      "metadata": {},
      "source": "\n### サイドノート\n\n人々は、GPU が高速であることを期待して、機械学習を行うために GPU を使用します。ただし、デバイス間での変数の転送には時間がかかります。したがって、私たちは、何かをゆっくりと実行したいと考えていることを 100% 確信してから実行してもらいたいと考えています。深層学習フレームワークがクラ​​ッシュせずにコピーを自動的に実行しただけであれば、遅いコードを書いたことに気づかない可能性があります。\n\nまた、デバイス (CPU、GPU、その他のマシン) 間のデータ転送は、計算よりもはるかに遅くなります。また、さらに操作を進める前にデータが送信される (または受信される) まで待たなければならないため、並列化がさらに困難になります。このため、コピー操作は細心の注意を払って行う必要があります。経験則として、多くの小さな操作は 1 つの大きな操作よりもはるかに悪いです。さらに、何をやっているのかよくわかっていない限り、一度に複数の操作を実行するほうが、コード内に多数の単一操作を散在させるよりもはるかに優れています。これは、一方のデバイスが他のことを行う前に他方のデバイスを待機する必要がある場合、そのような操作がブロックされる可能性があるためです。これは、電話でコーヒーを事前注文して、準備ができていることがわかるのではなく、行列に並んでコーヒーを注文するのと似ています。\n\n最後に、テンソルを出力するとき、またはテンソルを NumPy 形式に変換するときに、データがメイン メモリにない場合、フレームワークは最初にデータをメイン メモリにコピーするため、追加の送信オーバーヘッドが発生します。さらに悪いことに、Python が完了するまですべてを待たせる恐ろしいグローバル インタプリタ ロックの対象になっています。\n\n##  [**ニューラルネットワークとGPU** ]\n\n同様に、ニューラル ネットワーク モデルはデバイスを指定できます。次のコードは、モデル パラメーターを GPU に配置します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8d4f7b98",
      "metadata": {},
      "outputs": [],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(1))\n",
        "net = net.to(device=try_gpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7097e254",
      "metadata": {},
      "source": "\n次の章では、GPU でモデルを実行する方法の例をさらに多く見ていきます。これは、単に計算量が多少多くなるからです。\n\n入力が GPU 上のテンソルである場合、モデルは同じ GPU 上で結果を計算します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "60735ce8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1746],\n",
              "        [0.1746]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd340a96",
      "metadata": {},
      "source": "\n(**モデルパラメータが同じ GPU に保存されていることを確認してみましょう。** )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "58ff9b66",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net[0].weight.data.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e378a88f",
      "metadata": {},
      "source": "\nトレーナーに GPU をサポートさせます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "45166355",
      "metadata": {},
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
        "    self.save_hyperparameters()\n",
        "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def prepare_batch(self, batch):\n",
        "    if self.gpus:\n",
        "        batch = [a.to(self.gpus[0]) for a in batch]\n",
        "    return batch\n",
        "\n",
        "@d2l.add_to_class(d2l.Trainer)  #@save\n",
        "def prepare_model(self, model):\n",
        "    model.trainer = self\n",
        "    model.board.xlim = [0, self.max_epochs]\n",
        "    if self.gpus:\n",
        "        model.to(self.gpus[0])\n",
        "    self.model = model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2025908",
      "metadata": {},
      "source": "\nつまり、すべてのデータとパラメーターが同じデバイス上にある限り、モデルを効率的に学習できます。次の章では、そのような例をいくつか見ていきます。\n\n## まとめ\n\nCPU や GPU など、ストレージと計算用のデバイスを指定できます。デフォルトでは、データはメイン メモリに作成され、CPU が計算に使用されます。ディープ ラーニング フレームワークでは、計算用のすべての入力データが同じデバイス (CPU または同じ GPU) 上に存在する必要があります。データを不注意に移動すると、パフォーマンスが大幅に低下する可能性があります。典型的な間違いは次のとおりです。GPU 上のすべてのミニバッチの損失を計算し、それをコマンド ラインでユーザーに報告する (または NumPy `ndarray`に記録する) と、グローバル インタプリタ ロックがトリガーされ、すべての GPU が停止します。 GPU 内のログ用にメモリを割り当て、より大きなログのみを移動する方がはるかに優れています。\n\n## 演習\n1. 大きな行列の乗算など、より大きな計算タスクを試して、CPU と GPU の速度の違いを確認してください。計算量が少ないタスクの場合はどうでしょうか?\n1.  GPU 上でモデル パラメーターを読み書きするにはどうすればよいでしょうか?\n1.  $100 \\times 100$ 行列の 1,000 回の行列間の乗算を計算し、出力行列のフロベニウス ノルムを一度に 1 つの結果ずつログに記録するのにかかる時間を、GPU にログを保持して最終結果のみを転送する場合と比べて測定します。\n1.  2 つの行列間の乗算を 2 つの GPU で同時に実行する場合と、1 つの GPU で順番に実行する場合にかかる時間を測定します。ヒント: ほぼ線形のスケーリングが表示されるはずです。\n"
    },
    {
      "cell_type": "markdown",
      "id": "aa7caee2",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/63)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}