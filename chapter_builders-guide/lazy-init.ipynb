{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eee35a8e",
      "metadata": {},
      "source": "\nこのノートブックを実行するには、次の追加ライブラリが必要です。 Colab での実行は実験的なものであることに注意してください。問題がある場合は、Github の問題を報告してください。\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58f56ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-beta0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e957ed",
      "metadata": {},
      "source": "\n# 遅延初期化\n\n:label: `sec_lazy_init`\n\nこれまでのところ、ネットワークの設定がずさんで済んだように見えるかもしれません。具体的には、次のような直感的ではないことを実行しましたが、機能するはずがないように見えるかもしれません。\n- 入力次元を指定せずにネットワーク アーキテクチャを定義しました。\n- 前のレイヤーの出力サイズを指定せずにレイヤーを追加しました。\n- モデルに含めるパラメーターの数を決定するのに十分な情報を提供する前に、これらのパラメーターを「初期化」することもありました。\n\n私たちのコードが実際に実行されることに驚かれるかもしれません。結局のところ、深層学習フレームワークがネットワークの入力次元が何になるかを知る方法はありません。ここでのトリックは、フレーム*ワークが初期化を延期し*、最初にモデルにデータを渡すまで待機して、各レイヤーのサイズをその場で推測することです。\n\nその後、畳み込みニューラル ネットワークを使用する場合、入力の次元 (つまり、画像の解像度) が後続の各層の次元に影響を与えるため、この手法はさらに便利になります。したがって、コードの作成時に次元が何であるかを知らなくてもパラメーターを設定できる機能により、モデルを指定してその後変更するタスクが大幅に簡素化されます。次に、初期化の仕組みをさらに詳しく見ていきます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6cc4a5c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c247de1",
      "metadata": {},
      "source": "\nまず、MLP をインスタンス化しましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b2f08cc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9026c78a",
      "metadata": {},
      "source": "\nこの時点では、入力次元が不明なままであるため、ネットワークは入力層の重みの次元を知ることができない可能性があります。\n"
    },
    {
      "cell_type": "markdown",
      "id": "81899cf8",
      "metadata": {},
      "source": "\nしたがって、フレームワークはまだパラメータを初期化していません。以下のパラメータにアクセスして確認します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0bb829a0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<UninitializedParameter>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net[0].weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e070d5",
      "metadata": {},
      "source": "\n次に、ネットワーク経由でデータを渡し、フレームワークが最終的にパラメータを初期化できるようにします。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "43b3ea8b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 20])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.rand(2, 20)\n",
        "net(X)\n",
        "\n",
        "net[0].weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc79f7d",
      "metadata": {},
      "source": "\n入力次元 20 がわかると、フレームワークは値 20 を代入することで最初の層の重み行列の形状を識別できます。最初の層の形状を認識すると、フレームワークは 2 番目の層に進み、以下同様に続きます。すべての形状が判明するまで計算グラフを作成します。この場合、最初の層のみ遅延初期化が必要ですが、フレームワークは順次初期化されることに注意してください。すべてのパラメータの形状が判明すると、フレームワークは最終的にパラメータを初期化できます。\n"
    },
    {
      "cell_type": "markdown",
      "id": "51a93ab1",
      "metadata": {},
      "source": "\n次のメソッドは、すべてのパラメーター形状を推測するためのドライ ランのためにネットワークを介してダミー入力を渡し、その後パラメーターを初期化します。これは、後でデフォルトのランダムな初期化が望ましくない場合に使用されます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b557b602",
      "metadata": {},
      "outputs": [],
      "source": [
        "@d2l.add_to_class(d2l.Module)  #@save\n",
        "def apply_init(self, inputs, init=None):\n",
        "    self.forward(*inputs)\n",
        "    if init is not None:\n",
        "        self.net.apply(init)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b38844",
      "metadata": {},
      "source": "\n## まとめ\n\n遅延初期化は便利で、フレームワークがパラメーターの形状を自動的に推測できるようになり、アーキテクチャの変更が容易になり、一般的なエラーの原因が 1 つ排除されます。モデルを介してデータを渡して、フレームワークが最終的にパラメーターを初期化できるようにします。\n\n## 演習\n1. 最初のレイヤーには入力寸法を指定し、後続のレイヤーには指定しなかった場合はどうなりますか?すぐに初期化されますか?\n1. 一致しない寸法を指定するとどうなりますか?\n1. さまざまな次元の入力がある場合、何をする必要があるでしょうか?ヒント: パラメーターの結合を見てください。\n"
    },
    {
      "cell_type": "markdown",
      "id": "1809c73c",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/8092)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}