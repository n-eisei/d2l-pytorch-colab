{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9601b0a",
      "metadata": {},
      "source": "\n# パラメータ管理\n\nアーキテクチャを選択してハイパーパラメータを設定したら、トレーニング ループに進みます。ここでの目標は、損失関数を最小化するパラメータ値を見つけることです。トレーニング後、将来の予測を行うためにこれらのパラメーターが必要になります。さらに、他のコンテキストでパラメータを再利用したり、他のソフトウェアで実行できるようにモデルをディスクに保存したり、科学的な理解を得るために検査するためにパラメータを抽出したい場合もあります。\n\nほとんどの場合、パラメータの宣言と操作の詳細については無視して、ディープ ラーニング フレームワークに頼って面倒な作業を行うことができます。ただし、標準レイヤーを備えたスタック型アーキテクチャから離れると、パラメーターの宣言と操作という雑草に手を染める必要が生じることがあります。このセクションでは、次の内容について説明します。\n- デバッグ、診断、視覚化のためのパラメータへのアクセス。\n- 異なるモデルコンポーネント間でパラメータを共有します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "49ad8630",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52671949",
      "metadata": {},
      "source": "\n(**まず、1 つの隠れ層を持つ MLP に焦点を当てます。** )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "81e2ba98",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 1])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = nn.Sequential(nn.LazyLinear(8),\n",
        "                    nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "X = torch.rand(size=(2, 4))\n",
        "net(X).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2de5a28",
      "metadata": {},
      "source": "\n## [**パラメータアクセス**]\n\n :label: `subsec_param-access`\n\nすでに知っているモデルからパラメーターにアクセスする方法から始めましょう。\n"
    },
    {
      "cell_type": "markdown",
      "id": "c54d12f0",
      "metadata": {},
      "source": "\nモデルが`Sequential`クラスを介して定義されている場合、リストであるかのようにモデルにインデックスを付けることで、最初に任意のレイヤーにアクセスできます。各レイヤーのパラメーターは、その属性に簡単に配置できます。\n"
    },
    {
      "cell_type": "markdown",
      "id": "32fedc5a",
      "metadata": {},
      "source": "\n次のようにして、2 番目の全結合層のパラメーターを検査できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "097d676f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('weight',\n",
              "              tensor([[-0.2523,  0.2104,  0.2189, -0.0395, -0.0590,  0.3360, -0.0205, -0.1507]])),\n",
              "             ('bias', tensor([0.0694]))])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net[2].state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ba0725",
      "metadata": {},
      "source": "\nこの全結合層には、その層の重みとバイアスにそれぞれ対応する 2 つのパラメーターが含まれていることがわかります。\n\n###  【**対象パラメータ**】\n\n各パラメータはパラメータ クラスのインスタンスとして表されることに注意してください。パラメータを使って何か役立つことをするには、まず基礎となる数値にアクセスする必要があります。これを行うにはいくつかの方法があります。より単純なものもあれば、より一般的なものもあります。次のコードは、パラメーター クラスのインスタンスを返し、さらにそのパラメーターの値にアクセスする 2 番目のニューラル ネットワーク層からバイアスを抽出します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ba9bafe4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.nn.parameter.Parameter, tensor([0.0694]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(net[2].bias), net[2].bias.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8acd305f",
      "metadata": {},
      "source": "\nパラメータは、値、勾配、追加情報を含む複雑なオブジェクトです。このため、値を明示的にリクエストする必要があります。\n\n値に加えて、各パラメーターを使用して勾配にアクセスすることもできます。このネットワークに対してバックプロパゲーションをまだ呼び出していないため、初期状態にあります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d4a7c16a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net[2].weight.grad == None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0905e5e",
      "metadata": {},
      "source": "\n### [**すべてのパラメータを一度に**]\n\nすべてのパラメータに対して操作を実行する必要がある場合、パラメータに 1 つずつアクセスするのは面倒になる可能性があります。より複雑なモジュール (例: ネストされたモジュール) を扱う場合、各サブモジュールのパラメータを抽出するためにツリー全体を再帰する必要があるため、状況は特に扱いにくくなる可能性があります。以下では、すべてのレイヤーのパラメーターにアクセスする方法を示します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "997cac22",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('0.weight', torch.Size([8, 4])),\n",
              " ('0.bias', torch.Size([8])),\n",
              " ('2.weight', torch.Size([1, 8])),\n",
              " ('2.bias', torch.Size([1]))]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[(name, param.shape) for name, param in net.named_parameters()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9170b01c",
      "metadata": {},
      "source": "\n## [**関連付けられたパラメータ**]\n\n多くの場合、複数のレイヤー間でパラメータを共有したいことがあります。これをエレガントに行う方法を見てみましょう。以下では、完全に接続されたレイヤーを割り当て、そのパラメーターを特に使用して別のレイヤーのパラメーターを設定します。ここでは、パラメータにアクセスする前に順伝播`net(X)`を実行する必要があります。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b352ab5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([True, True, True, True, True, True, True, True])\n",
            "tensor([True, True, True, True, True, True, True, True])\n"
          ]
        }
      ],
      "source": [
        "# We need to give the shared layer a name so that we can refer to its\n",
        "# parameters\n",
        "shared = nn.LazyLinear(8)\n",
        "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    shared, nn.ReLU(),\n",
        "                    nn.LazyLinear(1))\n",
        "\n",
        "net(X)\n",
        "# Check whether the parameters are the same\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
        "net[2].weight.data[0, 0] = 100\n",
        "# Make sure that they are actually the same object rather than just having the\n",
        "# same value\n",
        "print(net[2].weight.data[0] == net[4].weight.data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf5f552",
      "metadata": {},
      "source": "\nこの例は、2 番目と 3 番目のレイヤーのパラメーターが関連付けられていることを示しています。それらは単に等しいだけではなく、まったく同じテンソルで表されます。したがって、パラメータの 1 つを変更すると、他のパラメータも変更されます。\n"
    },
    {
      "cell_type": "markdown",
      "id": "784d632c",
      "metadata": {},
      "source": "\nパラメーターが関連付けられると、グラデーションはどうなるのかと疑問に思うかもしれません。モデル パラメーターには勾配が含まれているため、2 番目の隠れ層と 3 番目の隠れ層の勾配はバックプロパゲーション中に加算されます。\n"
    },
    {
      "cell_type": "markdown",
      "id": "907cce9a",
      "metadata": {},
      "source": "\n## まとめ\n\nモデルパラメータにアクセスして結び付ける方法はいくつかあります。\n\n## 演習\n1. :numref: `sec_model_construction`で定義された`NestMLP`モデルを使用し、さまざまなレイヤーのパラメーターにアクセスします。\n1. 共有パラメータ層を含む MLP を構築し、トレーニングします。トレーニング プロセス中に、各レイヤーのモデル パラメーターと勾配を観察します。\n1. パラメーターを共有することがなぜ良い考えなのでしょうか?\n"
    },
    {
      "cell_type": "markdown",
      "id": "c8764e9d",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/57)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}