{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73c08b12",
      "metadata": {},
      "source": "\n# 最適化アルゴリズム\n\n:label: `chap_optimization`\n\nこの時点までこの本を順番に読んだ場合は、深層学習モデルをトレーニングするためにすでに多数の最適化アルゴリズムを使用していることになります。これらは、モデル パラメーターの更新を継続し、トレーニング セットで評価された損失関数の値を最小限に抑えることを可能にするツールでした。実際、最適化を単純な設定で目的関数を最小化するブラックボックス装置として扱うことに満足している人は、そのような手順の一連の呪文 (「SGD」や「Adam」などの名前) が存在することを知って満足するかもしれません。 ）。\n\nただし、うまくやるには、より深い知識が必要です。最適化アルゴリズムはディープラーニングにとって重要です。一方で、複雑な深層学習モデルのトレーニングには、数時間、数日、さらには数週間かかる場合もあります。最適化アルゴリズムのパフォーマンスは、モデルのトレーニング効率に直接影響します。一方、さまざまな最適化アルゴリズムの原理とそのハイパーパラメーターの役割を理解することで、目的を絞った方法でハイパーパラメーターを調整して深層学習モデルのパフォーマンスを向上させることができます。\n\nこの章では、一般的なディープラーニング最適化アルゴリズムについて詳しく説明します。深層学習で発生するほとんどすべての最適化問題は*非凸*です。それにもかかわらず、*凸*問題のコンテキストにおけるアルゴリズムの設計と分析は、非常に有益であることが証明されています。この章に凸の最適化に関する入門書と、凸の目的関数に対する非常に単純な確率的勾配降下法アルゴリズムの証明が含まれているのはそのためです。\n\n :begin_tab:toc\n- [最適化の概要](optimization-intro.ipynb)\n- [凸面](convexity.ipynb)\n- [ああ](gd.ipynb)\n- [スウェーデン語](sgd.ipynb)\n- [ミニバッチ-SD](minibatch-sgd.ipynb)\n- [勢い](momentum.ipynb)\n- [アダグラド](adagrad.ipynb)\n- [rmsprop](rmsprop.ipynb)\n- [アダデルタ](adadelta.ipynb)\n- [アダム](adam.ipynb)\n- [lr-scheduler](lr-scheduler.ipynb) :end_tab:\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}