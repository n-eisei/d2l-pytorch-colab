{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "85606231",
      "metadata": {},
      "source": "\nこのノートブックを実行するには、次の追加ライブラリが必要です。 Colab での実行は実験的なものであることに注意してください。問題がある場合は、Github の問題を報告してください。\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a672376",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0-beta0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcb1ed89",
      "metadata": {},
      "source": "\n# 転置畳み込み\n\n:label: `sec_transposed_conv`\n\nこれまで見てきた畳み込み層 (:numref: `sec_conv_layer` ) やプーリング層 (:numref: `sec_pooling` ) などの CNN 層は、通常、入力の空間次元 (高さと幅) を削減 (ダウンサンプリング) するか、変更しないままにします。 。ピクセルレベルで分類するセマンティックセグメンテーションでは、入力と出力の空間次元が同じであると便利です。たとえば、1 つの出力ピクセルのチャネル次元は、同じ空間位置にある入力ピクセルの分類結果を保持できます。\n\nこれを達成するには、特に CNN 層によって空間次元が削減された後、中間特徴マップの空間次元を増加 (アップサンプリング) できる別のタイプの CNN 層を使用できます。このセクションでは、*畳み込みによるダウンサンプリング操作を逆転するための、転置**畳み込み (分数ストライド畳み込み*とも呼ばれます:cite: `Dumoulin.Visin.2016` ) を紹介します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "111877b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5828aeb4",
      "metadata": {},
      "source": "\n## 基本操作\n\nここではチャネルを無視して、ストライド 1 でパディングなしの基本的な転置畳み込み演算から始めましょう。 $n_h \\times n_w$ 入力テンソルと $k_h \\times k_w$ カーネルが与えられたとします。各行で $n_w$ 回、各列で $n_h$ 回、ストライド 1 でカーネル ウィンドウをスライドすると、合計 $n_h n_w$ の中間結果が得られます。各中間結果は、ゼロとして初期化される $(n_h + k_h - 1) \\times (n_w + k_w - 1)$ テンソルです。各中間テンソルを計算するには、入力テンソルの各要素がカーネルで乗算され、結果として得られる $k_h \\times k_w$ テンソルが各中間テンソルの一部を置き換えます。各中間テンソル内の置換された部分の位置は、計算に使用される入力テンソル内の要素の位置に対応することに注意してください。最終的に、すべての中間結果が合計されて出力が生成されます。\n\n例として、:numref: `fig_trans_conv` 、$2\\times 2$ 入力テンソルに対して $2\\times 2$ カーネルによる転置畳み込みがどのように計算されるかを示しています。 \n\n![](http://d2l.ai/_images/trans_conv.svg) :label: `fig_trans_conv`\n\n入力行列`X`とカーネル行列`K`に対して`trans_conv`実行できます (**この基本的な転置畳み込み演算を実装できます**)。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ddd262ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "def trans_conv(X, K):\n",
        "    h, w = K.shape\n",
        "    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
        "    for i in range(X.shape[0]):\n",
        "        for j in range(X.shape[1]):\n",
        "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc96ae42",
      "metadata": {},
      "source": "\nカーネルを介して入力要素を*減らす*通常の畳み込み (:numref: `sec_conv_layer`内) とは対照的に、転置畳み込みはカーネルを介して入力要素を*ブロードキャストする*ため、入力よりも大きな出力が生成されます。 :numref: `fig_trans_conv`から入力テンソル`X`とカーネル テンソル`K`を構築して、基本的な 2 次元転置畳み込み**演算の [上記の実装の出力を検証**] することができます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c1bd7c17",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.,  0.,  1.],\n",
              "        [ 0.,  4.,  6.],\n",
              "        [ 4., 12.,  9.]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "trans_conv(X, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75253849",
      "metadata": {},
      "source": "\nあるいは、入力`X`とカーネル`K`両方とも 4 次元テンソルである場合、[**高レベル API を使用して同じ結果を得る**] ことができます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5065f143",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  0.,  1.],\n",
              "          [ 0.,  4.,  6.],\n",
              "          [ 4., 12.,  9.]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3422a8",
      "metadata": {},
      "source": "\n## [**パディング、ストライド、および複数のチャネル**]\n\nパディングが入力に適用される通常の畳み込みとは異なり、転置畳み込みではパディングが出力に適用されます。たとえば、高さと幅の両側のパディング番号を 1 に指定すると、最初と最後の行と列が転置された畳み込み出力から削除されます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d12f97a1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036423cb",
      "metadata": {},
      "source": "\n転置畳み込みでは、ストライドは入力ではなく中間結果 (つまり出力) に対して指定されます。 :numref: `fig_trans_conv`からの同じ入力テンソルとカーネル テンソルを使用して、ストライドを 1 から 2 に変更すると、中間テンソルの高さと重みの両方が増加するため、 :numref: `fig_trans_conv_stride2`の出力テンソルが増加します。 \n\n![](http://d2l.ai/_images/trans_conv_stride2.svg) :label: `fig_trans_conv_stride2`\n\n次のコード スニペットは、 :numref: `fig_trans_conv_stride2`のストライド 2 の転置畳み込み出力を検証できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5af4db7d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0., 1.],\n",
              "          [0., 0., 2., 3.],\n",
              "          [0., 2., 0., 3.],\n",
              "          [4., 6., 6., 9.]]]], grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
        "tconv.weight.data = K\n",
        "tconv(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b37f9ef",
      "metadata": {},
      "source": "\n複数の入力および出力チャンネルの場合、転置畳み込みは通常の畳み込みと同じように機能します。入力に ​​$c_i$ チャネルがあり、転置畳み込みによって $k_h\\times k_w$ カーネル テンソルが各入力チャネルに割り当てられるとします。複数の出力チャネルが指定されている場合、出力チャネルごとに $c_i\\times k_h\\times k_w$ カーネルが存在します。\n\n全体として、$\\mathsf{X}$ を畳み込み層 $f$ に入力して $\\mathsf{Y}=f(\\mathsf{X})$ を出力し、転置畳み込み層 $g$ を作成すると、出力チャネルの数が $\\mathsf{X}$ のチャネル数であることを除き、$f$ と同じハイパーパラメータの場合、$g(Y)$ は $\\mathsf{X}$ と同じ形状になります。これは次の例で説明できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b3abd75",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.rand(size=(1, 10, 16, 16))\n",
        "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
        "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
        "tconv(conv(X)).shape == X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e37948d9",
      "metadata": {},
      "source": "\n## 【**マトリックストランスポーズとの接続**】\n\n :label: `subsec-connection-to-mat-transposition`\n\n転置畳み込みは、行列転置にちなんで名付けられました。説明するために、まず行列の乗算を使用して畳み込みを実装する方法を見てみましょう。以下の例では、 $3\\times 3$ 入力`X`と $2\\times 2$ 畳み込みカーネル`K`を定義し、関数`corr2d`を使用して畳み込み出力`Y`を計算します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "36e85bcc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[27., 37.],\n",
              "        [57., 67.]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = torch.arange(9.0).reshape(3, 3)\n",
        "K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "Y = d2l.corr2d(X, K)\n",
        "Y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a57892a8",
      "metadata": {},
      "source": "\n次に、コンボリューション カーネル`K`多くのゼロを含む疎な重み行列`W`として書き換えます。重み行列の形状は ($4$, $9$) で、非ゼロ要素は畳み込みカーネル`K`から取得されます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "931844bd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 0., 3., 4., 0., 0., 0., 0.],\n",
              "        [0., 1., 2., 0., 3., 4., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 2., 0., 3., 4., 0.],\n",
              "        [0., 0., 0., 0., 1., 2., 0., 3., 4.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def kernel2matrix(K):\n",
        "    k, W = torch.zeros(5), torch.zeros((4, 9))\n",
        "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
        "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
        "    return W\n",
        "\n",
        "W = kernel2matrix(K)\n",
        "W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8fb7031",
      "metadata": {},
      "source": "\n入力`X`を行ごとに連結して、長さ 9 のベクトルを取得します。次に、 `W`とベクトル化された`X`の行列乗算により、長さ 4 のベクトルが得られます。それを再形成すると、上記の元の畳み込み演算から同じ結果`Y`を取得できます。行列の乗算を使用して畳み込みを実装しただけです。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3c6b35df",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[True, True],\n",
              "        [True, True]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y == torch.matmul(W, X.reshape(-1)).reshape(2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e443af",
      "metadata": {},
      "source": "\n同様に、行列の乗算を使用して転置畳み込みを実装できます。次の例では、上記の通常の畳み込みからの $2 \\times 2$ 出力`Y` 、転置畳み込みへの入力として取得します。行列を乗算することでこの演算を実装するには、重み行列`W`を新しい形状 $(9, 4)$ に転置するだけで済みます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "371d488c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Z = trans_conv(Y, K)\n",
        "Z == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485cb078",
      "metadata": {},
      "source": "\n行列を乗算して畳み込みを実装することを検討してください。入力ベクトル $\\mathbf{x}$ と重み行列 $\\mathbf{W}$ が与えられると、その入力に重み行列を乗算し、ベクトル $\\mathbf{y を出力することで、畳み込みの順伝播関数を実装できます。 }=\\mathbf{W}\\mathbf{x}$。逆伝播は連鎖則と $\\nabla_{\\mathbf{x}}\\mathbf{y}=\\mathbf{W}^\\top$ に従うため、畳み込みの逆伝播関数は入力に転置された重みを乗算することで実装できます。行列 $\\mathbf{W}^\\top$。したがって、転置畳み込み層は、畳み込み層の順伝播関数と逆伝播関数を交換するだけで済みます。その順伝播関数と逆伝播関数は、入力ベクトルに $\\mathbf{W}^\\top$ と $\\mathbf{W} を乗算します。 $、それぞれ。\n\n## まとめ\n- カーネルを介して入力要素を減らす通常の畳み込みとは対照的に、転置畳み込みはカーネルを介して入力要素をブロードキャストするため、入力よりも大きな出力が生成されます。\n-  $\\mathsf{X}$ を畳み込み層 $f$ に入力して $\\mathsf{Y}=f(\\mathsf{X})$ を出力し、$ と同じハイパーパラメータを持つ転置畳み込み層 $g$ を作成するとします。 $\\mathsf{X}$ のチャンネル数である出力チャンネル数を除いて f$ の場合、$g(Y)$ は $\\mathsf{X}$ と同じ形状になります。\n- 行列の乗算を使用して畳み込みを実装できます。転置畳み込み層は、畳み込み層の順伝播関数と逆伝播関数を交換するだけで済みます。\n\n## 演習\n1. :numref: `subsec-connection-to-mat-transposition`では、畳み込み入力`X`と転置された畳み込み出力`Z`同じ形状になります。それらは同じ価値を持っていますか?なぜ？\n1. 畳み込みを実装するために行列の乗算を使用するのは効率的ですか?なぜ？\n"
    },
    {
      "cell_type": "markdown",
      "id": "791007d5",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/1450)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}