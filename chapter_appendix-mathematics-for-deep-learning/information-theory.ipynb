{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4bd0fd1d",
      "metadata": {},
      "source": "\n# 情報理論\n\n:label: `sec_information_theory`\n\n宇宙には情報が溢れています。情報は、学問分野の亀裂を超えて共通言語を提供します。シェイクスピアのソネットからコーネル大学 ArXiv に関する研究者の論文に至るまで、ゴッホの印刷版『星月夜』からベートーベンの音楽交響曲第 5 番に至るまで、最初のプログラミング言語プランカルキュルから最先端のマシンに至るまでです。学習アルゴリズム。形式に関係なく、すべては情報理論のルールに従わなければなりません。情報理論を使用すると、さまざまな信号にどの程度の情報が存在するかを測定して比較できます。このセクションでは、情報理論の基本概念と機械学習における情報理論の応用について調査します。\n\n始める前に、機械学習と情報理論の関係について概説しましょう。機械学習は、データから興味深い信号を抽出し、重要な予測を行うことを目的としています。一方、情報理論では、情報のエンコード、デコード、送信、操作を研究します。その結果、情報理論は、機械学習システムにおける情報処理を議論するための基本的な言語を提供します。たとえば、多くの機械学習アプリケーションは、 :numref: `sec_softmax`で説明されているように、クロスエントロピー損失を使用します。この損失は、情報理論の考察から直接導き出すことができます。\n\n## 情報\n\n情報理論の「魂」である情報から始めましょう。*情報は、* 1 つ以上のエンコード形式の特定のシーケンスを使用して任意にエンコードできます。私たちが情報の概念を定義するという課題を自分自身に課したとします。私たちの出発点は何でしょうか?\n\n次の思考実験を考えてみましょう。私たちの友達にトランプを持っている人がいます。彼らはデッキをシャッフルし、いくつかのカードを裏返し、カードについての説明を私たちに話します。私たちは各ステートメントの情報内容を評価しようとします。\n\nまず、カードをめくって「カードが見えます」と言います。これではまったく情報が得られません。これが事実であることはすでに確信していたため、情報がゼロであることを願っています。\n\n次に、カードを裏返して「ハートが見えます」と言います。これにより、いくつかの情報が得られますが、実際には、可能性のある異なるスーツは $4$ しかなく、それぞれの可能性が等しいため、この結果には驚きません。情報の尺度が何であれ、このイベントの情報量は少なくなるようにしたいと考えています。\n\n次に、カードを裏返して、「これはスペードの 3 ドルです」と言います。これは詳しい情報です。確かに、同様に可能性の高い $52$ の結果があり、私たちの友人はそれがどれであるかを教えてくれました。これは中程度の情報量である必要があります。\n\nこれを論理的に極限まで考えてみましょう。最後に、山札からすべてのカードを裏返し、シャッフルされた山札全体のシーケンスを読み上げたとします。デッキには $52!$ の異なるオーダーがあり、これもすべて同じ確率であるため、どれであるかを知るには多くの情報が必要です。\n\n私たちが開発する情報の概念はすべて、この直観に従わなければなりません。実際、次のセクションでは、これらのイベントが $0\\text{ bits}$、$2\\text{ bits}$、$~5.7\\text{ bits}$、および $~225.6\\text{ bits を持つことを計算する方法を学びます。それぞれ }$ の情報。\n\nこれらの思考実験を読んでみると、自然なアイデアが見えてきます。出発点として、私たちは知識を気にするのではなく、情報は驚きの程度や出来事の抽象的な可能性を表すという考えを構築するかもしれません。たとえば、異常な出来事を説明したい場合、多くの情報が必要になります。一般的なイベントの場合、多くの情報は必要ないかもしれません。\n\n 1948 年に、クロード E. シャノンは、情報理論を確立する*『コミュニケーションの数学理論』*を出版しました:cite: `Shannon.1948` 。シャノンはその記事の中で、情報エントロピーの概念を初めて導入しました。ここから旅を始めます。\n\n### 自己情報\n\n情報はイベントの抽象的な可能性を具体化するため、その可能性をビット数にどのようにマッピングするのでしょうか?シャノンは、ジョン テューキーによって最初に作成された情報の単位として*ビットという*用語を導入しました。では、「ビット」とは何ですか?なぜ情報を測定するためにビットを使用するのでしょうか?歴史的に、アンティークの送信機は $0$ と $1$ の 2 種類のコードしか送受信できませんでした。実際、バイナリ エンコーディングは、現代のすべてのデジタル コンピュータで依然として一般的に使用されています。このように、あらゆる情報は一連の $0$ と $1$ によってエンコードされます。したがって、長さ $n$ の一連の 2 進数には、$n$ ビットの情報が含まれます。\n\nここで、任意の一連のコードについて、それぞれの $0$ または $1$ が $\\frac{1}{2}$ の確率で発生すると仮定します。したがって、長さ $n$ の一連のコードを持つイベント $X$ は、$\\frac{1}{2^n}$ の確率で発生します。同時に、前に述べたように、このシリーズには $n$ ビットの情報が含まれています。では、確率 $p$ をビット数に変換できる数学関数に一般化できるでしょうか?シャノンは*自己情報を*定義することで答えを出しました\n\n$$I(X) = - \\log_2 (p),$$\n\nこのイベント $X$ に関して私たちが受け取った*情報*として。このセクションでは常に底 2 の対数を使用することに注意してください。わかりやすくするために、このセクションの残りの部分では対数表記の下付き文字 2 を省略します。つまり、$\\log(.)$ は常に $\\log_2(.)$ を指します。たとえば、コード「0010」には自己情報が含まれています。\n\n $$I(\\text{\"0010\"}) = - \\log (p(\\text{\"0010\"})) = - \\log \\left( \\frac{1}{2^4} \\right) = 4 \\text{ ビット}.$$\n\n自己情報は以下のように計算できます。その前に、まずこのセクションで必要なパッケージをすべてインポートしましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d05e3462",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn import NLLLoss\n",
        "\n",
        "\n",
        "def nansum(x):\n",
        "    # Define nansum, as pytorch does not offer it inbuilt.\n",
        "    return x[~torch.isnan(x)].sum()\n",
        "\n",
        "def self_information(p):\n",
        "    return -torch.log2(torch.tensor(p)).item()\n",
        "\n",
        "self_information(1 / 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de11e2ae",
      "metadata": {},
      "source": "\n## エントロピ\n\n自己情報は単一の離散イベントの情報のみを測定するため、離散分布または連続分布の確率変数に対するより一般化された測定が必要です。\n\n### 動機付けのエントロピー\n\n何が欲しいのか具体的に考えてみましょう。これは、*シャノンのエントロピーの公理*として知られるものについての非公式な記述になります。以下の常識的な記述の集まりは、私たちに情報の一意の定義を強制することがわかります。これらの公理の正式版は、他のいくつかの公理とともに :citet: `Csiszar.2008`にあります。\n1. 確率変数を観察することで得られる情報は、要素と呼ばれるものや、確率が 0 である追加要素の存在には依存しません。\n1.  2 つの確率変数を観察することで得られる情報は、それらを個別に観察して得られる情報の合計にすぎません。それらが独立している場合、それはまさに合計になります。\n1.  （ほぼ）特定のイベントを観察するときに得られる情報は（ほぼ）ゼロです。\n\nこの事実を証明することは本文の範囲を超えていますが、これがエントロピーが取るべき形式を一意に決定することを知っておくことが重要です。これらによって許容される唯一のあいまいさは、基本単位の選択にあります。これは、ほとんどの場合、1 回の公正なコイン投げによって提供される情報が 1 ビットであるという前に見た選択を行うことによって正規化されます。\n\n### 意味\n\n確率密度関数 (pdf) または確率質量関数 (pmf) $p(x)$ を使用した確率分布 $P$ に従う確率変数 $X$ について、*エントロピー*(または*シャノン)*を通じて期待される情報量を測定します。*エントロピー*)\n\n $$H(X) = - E_{x \\sim P} [\\log p(x)].$$ :eqlabel: `eq_ent_def`\n\n具体的には、$X$ が離散の場合、 $$H(X) = - \\sum_i p_i \\log p_i \\text{、ここで } p_i = P(X_i).$$\n\nそれ以外の場合、$X$ が連続の場合、エントロピーを*微分エントロピー*とも呼びます\n\n$$H(X) = - \\int_x p(x) \\log p(x) ; dx.$$\n\nエントロピーは次のように定義できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c3ce40b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6855)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def entropy(p):\n",
        "    entropy = - p * torch.log2(p)\n",
        "    # Operator `nansum` will sum up the non-nan number\n",
        "    out = nansum(entropy)\n",
        "    return out\n",
        "\n",
        "entropy(torch.tensor([0.1, 0.5, 0.1, 0.3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77afe4c9",
      "metadata": {},
      "source": "\n### 解釈\n\n興味があるかもしれません: エントロピー定義 :eqref: `eq_ent_def`で、なぜ負の対数の期待値を使用するのでしょうか?ここにいくつかの直感があります。\n\nまず、なぜ*対数*関数 $\\log$ を使うのでしょうか? $p(x) = f_1(x) f_2(x) \\ldots, f_n(x)$ とします。ここで、各成分関数 $f_i(x)$ は互いに独立しています。これは、各 $f_i(x)$ が、$p(x)$ から得られる情報全体に独立して寄与することを意味します。上で説明したように、エントロピー式が独立した確率変数に対して加算されることが必要です。幸いなことに、$\\log$ は確率分布の積を個々の項の合計に自然に変換できます。\n\n次に、なぜ*負の*$\\log$ を使用するのでしょうか?直感的には、頻度の高いイベントには、あまり一般的でないイベントよりも含まれる情報が少なくなるはずです。これは、通常のイベントよりも珍しいケースからより多くの情報を得ることが多いためです。ただし、$\\log$ は確率とともに単調増加し、$[0, 1]$ のすべての値が実際に負になります。私たちは、事象の確率とそのエントロピーとの間に単調減少する関係を構築する必要があります。この関係は理想的には常に正になります (なぜなら、私たちが観察したものによって、私たちが知っていることを忘れることを強制されるべきではないからです)。したがって、$\\log$ 関数の前にマイナス記号を追加します。\n\n最後に、*期待*関数はどこから来るのでしょうか?確率変数 $X$ について考えてみましょう。私たちは自己情報 ($-\\log(p)$) を、特定の結果を見たときに抱く*驚き*の量として解釈できます。実際、確率がゼロに近づくにつれて、驚きは無限大になります。同様に、エントロピーを $X$ の観察から得られる驚きの平均量として解釈できます。たとえば、スロット マシン システムが統計的に独立したシンボル ${s_1, \\ldots, s_k}$ を確率 ${p_1, \\ldots, p_k}$ でそれぞれ発行すると想像してください。このとき、このシステムのエントロピーは、各出力の観察から得られる平均的な自己情報に等しくなります。\n\n $$H(S) = \\sum_i {p_i \\cdot I(s_i)} = - \\sum_i {p_i \\cdot \\log p_i}.$$\n\n### エントロピーの性質\n\n上記の例と解釈により、 entropy :eqref: `eq_ent_def`の次のプロパティを導き出すことができます。ここで、$X$ をイベントと呼び、$P$ を $X$ の確率分布と呼びます。\n- すべての離散 $X$ に対して $H(X) \\geq 0$ (連続 $X$ に対してエントロピーは負になる可能性があります)。\n-  $X \\sim P$ を pdf または pmf $p(x)$ で計算し、pdf または pmf $q(x)$ を使用して新しい確率分布 $Q$ によって $P$ を推定しようとすると、次のようになります。 $$H(X) = - E_{x \\sim P} [\\log p(x)] \\leq - E_{x \\sim P} [\\log q(x)], \\text{ が等しい場合のみif } P = Q.$$ あるいは、$H(X)$ は、$P$ から抽出されたシンボルをエンコードするのに必要な平均ビット数の下限を与えます。\n-  $X \\sim P$ の場合、考えられるすべての結果に均等に分散すれば、$x$ は最大量の情報を伝えます。具体的には、確率分布 $P$ が $k$ クラス ${p_1, \\ldots, p_k }$ で離散的である場合、 $$H(X) \\leq \\log(k), \\text{ は等価になります。 Only if } p_i = \\frac{1}{k}, \\forall i.$$ $P$ が連続確率変数の場合、話はさらに複雑になります。ただし、$P$ が有限区間 ($0$ と $1$ の間のすべての値) でサポートされることを追加で課す場合、$P$ がその区間で一様分布である場合、$P$ は最も高いエントロピーを持ちます。\n\n## 相互情報\n\n以前、単一の確率変数 $X$ のエントロピーを定義しましたが、ペアの確率変数 $(X, Y)$ のエントロピーはどうなるでしょうか?これらのテクニックは、「$X$ と $Y$ にそれぞれ別々に含まれる情報と比較して、一緒に含まれる情報は何ですか? 冗長な情報はありますか? それともすべて固有の情報ですか?」というタイプの質問に答えようとしていると考えることができます。\n\n以下の議論では、常に $(X, Y)$ を確率変数のペアとして使用し、確率変数 $P$ と pdf または pmf $p_{X, Y}(x, y)$ を追跡します。一方、$X$ と $Y$ は、それぞれ確率分布 $p_X(x)$ と $p_Y(y)$ に従います。\n\n### 結合エントロピー\n\n単一の確率変数 :eqref: `eq_ent_def`のエントロピーと同様に、ペアの確率変数 $(X, Y)$ の*結合エントロピー*$H(X, Y)$ を次のように定義します。\n\n $$H(X, Y) = -E_{(x, y) \\sim P} [\\log p_{X, Y}(x, y)]。 $$ :eqlabel: `eq_joint_ent_def`\n\n正確に言うと、一方で $(X, Y)$ が離散確率変数のペアである場合、次のようになります。\n\n $$H(X, Y) = - \\sum_{x} \\sum_{y} p_{X, Y}(x, y) \\log p_{X, Y}(x, y).$$\n\n一方、$(X, Y)$ が連続確率変数のペアである場合、*微分結合エントロピーを*次のように定義します。\n\n $$H(X, Y) = - \\int_{x, y} p_{X, Y}(x, y) \\ \\log p_{X, Y}(x, y) ;dx ;dy.$$\n\n :eqref: `eq_joint_ent_def` 、確率変数のペアの合計のランダム性を示すものと考えることができます。極端なペアとして、$X = Y$ が 2 つの同一の確率変数である場合、ペアの情報は一方の情報とまったく同じであり、$H(X, Y) = H(X) = H(Y) となります。 $。逆に、$X$ と $Y$ が独立している場合、$H(X, Y) = H(X) + H(Y)$ となります。実際、確率変数のペアに含まれる情報は、いずれかの確率変数のエントロピー以上であり、両方の合計以下であることが常にわかります。\n\n $$ H(X), H(Y) \\le H(X, Y) \\le H(X) + H(Y)。 $$\n\n結合エントロピーを最初から実装してみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "190370ac",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6855)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def joint_entropy(p_xy):\n",
        "    joint_ent = -p_xy * torch.log2(p_xy)\n",
        "    # Operator `nansum` will sum up the non-nan number\n",
        "    out = nansum(joint_ent)\n",
        "    return out\n",
        "\n",
        "joint_entropy(torch.tensor([[0.1, 0.5], [0.1, 0.3]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1eec755",
      "metadata": {},
      "source": "\nこれは以前と同じ*コード*ですが、2 つの確率変数の結合分布を扱うものとして解釈が異なることに注意してください。\n\n### 条件付きエントロピー\n\n結合エントロピーは、確率変数のペアに含まれる情報量を超えて定義されます。これは便利ですが、多くの場合、私たちが気にするものではありません。機械学習の設定を考えてみましょう。 $X$ を画像のピクセル値を記述する確率変数 (または確率変数のベクトル) とし、$Y$ をクラス ラベルである確率変数とします。 $X$ には実質的な情報が含まれている必要があります。自然の画像は複雑なものです。ただし、画像が表示された後に $Y$ に含まれる情報は少なくなるはずです。実際、数字が判読できない場合を除き、数字の画像にはその数字に関する情報がすでに含まれているはずです。したがって、情報理論の語彙を拡張し続けるには、別の条件を条件とした確率変数の情報内容について推論できる必要があります。\n\n確率論では、変数間の関係を測定するための*条件付き確率*の定義を見てきました。ここで、同様に*条件付きエントロピー*$H(Y \\mid X)$ を定義したいと思います。これは次のように書くことができます\n\n$$ H(Y \\mid X) = - E_{(x, y) \\sim P} [\\log p(y \\mid x)],$$ :eqlabel: `eq_cond_ent_def`\n\nここで $p(y \\mid x) = \\frac{p_{X, Y}(x, y)}{p_X(x)}$ は条件付き確率です。具体的には、 $(X, Y)$ が離散確率変数のペアである場合、\n\n $$H(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log p(y \\mid x).$$\n\n $(X, Y)$ が連続確率変数のペアである場合、*微分条件付きエントロピー*は同様に次のように定義されます。\n\n $$H(Y \\mid X) = - \\int_x \\int_y p(x, y) \\ \\log p(y \\mid x) ;dx ;dy.$$\n\n*条件付きエントロピー*$H(Y \\mid X)$ がエントロピー $H(X)$ および結合エントロピー $H(X, Y)$ にどのような関係があるのか​​、という疑問が自然に生まれます。上記の定義を使用すると、これを明確に表現できます。\n\n $$H(Y \\mid X) = H(X, Y) - H(X).$$\n\nこれは直感的に解釈できます。$X$ が与えられた場合の $Y$ の情報 ($H(Y \\mid X)$) は、$X$ と $Y$ の両方の情報を合わせたものと同じです ($H(X, Y) )$) から $X$ に既に含まれている情報を差し引いたもの。これにより、$X$ では表されていない情報が $Y$ に得られます。\n\nここで、条件付きエントロピー :eqref: `eq_cond_ent_def`を最初から実装しましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3bc8f75c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.8635)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def conditional_entropy(p_xy, p_x):\n",
        "    p_y_given_x = p_xy/p_x\n",
        "    cond_ent = -p_xy * torch.log2(p_y_given_x)\n",
        "    # Operator `nansum` will sum up the non-nan number\n",
        "    out = nansum(cond_ent)\n",
        "    return out\n",
        "\n",
        "conditional_entropy(torch.tensor([[0.1, 0.5], [0.2, 0.3]]),\n",
        "                    torch.tensor([0.2, 0.8]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929581b1",
      "metadata": {},
      "source": "\n### 相互情報\n\n確率変数 $(X, Y)$ の以前の設定を考えると、次のように疑問に思うかもしれません。「$Y$ には含まれているが $X$ には含まれていない情報量がわかったので、同様に、どのくらいの情報が共有されているかを尋ねることができるでしょうか」 $X$と$Y$は？」答えは $(X, Y)$ の*相互情報量*であり、これを $I(X, Y)$ と書きます。\n\n正式な定義に直接飛び込むのではなく、最初に以前に作成した用語に完全に基づいて相互情報量の式を導出して、直観を練習してみましょう。 2 つの確率変数間で共有される情報を見つけたいと考えています。これを試みる 1 つの方法は、$X$ と $Y$ の両方に含まれるすべての情報を一緒に開始し、次に共有されていない部分を削除することです。 $X$ と $Y$ に含まれる情報を合わせて $H(X, Y)$ と書きます。ここから、$X$ には含まれるが $Y$ には含まれない情報と、$Y$ には含まれるが $X$ には含まれない情報を差し引く必要があります。前のセクションで見たように、これはそれぞれ $H(X \\mid Y)$ と $H(Y \\mid X)$ で与えられます。したがって、相互情報量は次のようになります。\n\n $$ I(X, Y) = H(X, Y) - H(Y \\mid X) - H(X \\mid Y)。 $$\n\n実際、これは相互情報量の有効な定義です。これらの用語の定義を拡張して組み合わせると、少しの代数でこれが次と同じであることがわかります。\n\n $$I(X, Y) = E_{x} E_{y} \\left{ p_{X, Y}(x, y) \\log\\frac{p_{X, Y}(x, y)}{p_X (x) p_Y(y)} \\right}。 $$ :eqlabel: `eq_mut_ent_def`\n\nこれらすべての関係は、 image :numref: `fig_mutual_information`に要約できます。これは、次のステートメントがすべて $I(X, Y)$ と同等である理由を理解するための優れた直観テストです。\n-  $H(X) - H(X \\mid Y)$\n-  $H(Y) - H(Y \\mid X)$\n-  $H(X) + H(Y) - H(X, Y)$ \n\n![](../img/mutual-information.svg) :label: `fig_mutual_information`\n\n多くの点で、相互情報量 :eqref: `eq_mut_ent_def` 、 :numref: `sec_random_variables`で見た相関係数の原則的な拡張として考えることができます。これにより、変数間の線形関係だけでなく、あらゆる種類の 2 つの確率変数間で共有される最大の情報を求めることができます。\n\nでは、相互情報を一から実装してみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "91b1e49c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7195)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def mutual_information(p_xy, p_x, p_y):\n",
        "    p = p_xy / (p_x * p_y)\n",
        "    mutual = p_xy * torch.log2(p)\n",
        "    # Operator `nansum` will sum up the non-nan number\n",
        "    out = nansum(mutual)\n",
        "    return out\n",
        "\n",
        "mutual_information(torch.tensor([[0.1, 0.5], [0.1, 0.3]]),\n",
        "                   torch.tensor([0.2, 0.8]), torch.tensor([[0.75, 0.25]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d98a740",
      "metadata": {},
      "source": "\n### 相互情報の性質\n\n相互情報量 :eqref: `eq_mut_ent_def`の定義を暗記するのではなく、その注目すべき特性だけを覚えておく必要があります。\n- 相互情報は対称です、つまり $I(X, Y) = I(Y, X)$ です。\n- 相互情報は負ではありません。つまり、$I(X, Y) \\geq 0$ です。\n-  $X$ と $Y$ が独立している場合に限り、$I(X, Y) = 0$。たとえば、$X$ と $Y$ が独立している場合、$Y$ を知っていても $X$ についての情報は得られず、その逆も同様であるため、相互情報量はゼロになります。\n- あるいは、$X$ が $Y$ の可逆関数である場合、$Y$ と $X$ はすべての情報を共有し、$$I(X, Y) = H(Y) = H(X).$$\n\n### 点ごとの相互情報\n\nこの章の冒頭でエントロピーを扱ったとき、特定の結果にどれほど*驚いたか*として $-\\log(p_X(x))$ の解釈を提供することができました。*点ごとの相互情報量と呼ばれる相互情報*量の対数項にも同様の解釈を与えることができます。\n\n $$\\mathrm{pmi}(x, y) = \\log\\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)}.$$ :eqlabel: `eq_pmi_def`\n\n :eqref: `eq_pmi_def` 、結果 $x$ と $y$ の特定の組み合わせが、独立したランダムな結果に対して期待されるものと比較してどの程度多かれ少なかれあるかを測定するものと考えることができます。それが大きくて正の場合、これら 2 つの特定の結果は、ランダムな偶然と比較した場合よりもはるかに頻繁に発生します (*注*: 分母は $p_X(x) p_Y(y)$ で、これは 2 つの結果が独立している確率です)。一方、それが大きく負の場合、ランダムな偶然によって予想されるよりもはるかに少ない 2 つの結果が発生していることを表します。\n\nこれにより、相互情報量 :eqref: `eq_mut_ent_def` 、2 つの結果が独立している場合に予想される結果と比較して、一緒に発生する結果を見て驚いた平均量として解釈できます。\n\n### 相互情報の適用\n\n相互情報は純粋な定義では少し抽象的かもしれませんが、機械学習とどのように関係するのでしょうか?自然言語処理において、最も困難な問題の 1 つは、*曖昧さの解決*、つまり文脈から単語の意味が不明瞭になる問題です。たとえば、最近のニュースの見出しは「アマゾンが炎上している」と報じました。アマゾンという会社の建物が燃えているのか、それともアマゾンの熱帯雨林が燃えているのか疑問に思うかもしれません。\n\nこの場合、相互情報がこの曖昧さを解決するのに役立ちます。まず、電子商取引、テクノロジー、オンラインなど、Amazon という企業との相互情報量が比較的大きい単語群を見つけます。次に、雨、森林、熱帯など、アマゾンの熱帯雨林との相互情報量が比較的大きい別の単語グループが見つかります。 「Amazon」を明確にする必要がある場合は、Amazon という単語の文脈でどのグループがより多く出現しているかを比較できます。この場合、記事は森林についての説明を続け、文脈を明確にします。\n\n## カルバックとライブラーの分岐\n\n:numref: `sec_linear-algebra`で説明したように、ノルムを使用して任意の次元の空間内の 2 点間の距離を測定できます。確率分布でも同様のタスクを実行できるようにしたいと考えています。これを行うには多くの方法がありますが、情報理論は最も優れた方法の 1 つを提供します。ここでは、2 つの分布が互いに近いかどうかを測定する方法を提供する、*カルバック・ライブラー (KL) 発散*を調べます。\n\n### 意味\n\npdf または pmf $p(x)$ を使用して確率分布 $P$ に従う確率変数 $X$ が与えられると、pdf または pmf を使用して別の確率分布 $Q$ によって $P$ を推定します $q( x)$。 $P$ と $Q$ の間の*カルバック・ライブラー (KL) 発散*(または*相対エントロピー*) は次のようになります。\n\n $$D_{\\mathrm{KL}}(P|Q) = E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right].$$ :eqlabel : `eq_kl_def`\n\n点ごとの相互情報量 :eqref: `eq_pmi_def`と同様に、対数項の解釈を再度提供できます: $-\\log \\frac{q(x)}{p(x)} = -\\log(q(x)) - (-\\log(p(x)))$ は、$Q$ で予想されるよりも $P$ の下で $x$ がはるかに頻繁に見られる場合、大きくてプラスになり、結果が遠くまで見える場合、大きくてマイナスになります。予想よりも少ない。このようにして、基準分布から結果を観察するときの驚きと比較して、結果を観察したときの*相対的な*驚きとして解釈できます。\n\n KL ダイバージェンスをスクラッチから実装してみましょう。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "07ba736c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def kl_divergence(p, q):\n",
        "    kl = p * torch.log2(p / q)\n",
        "    out = nansum(kl)\n",
        "    return out.abs().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6186c892",
      "metadata": {},
      "source": "\n### KL ダイバージェンス プロパティ\n\nKL 発散 :eqref: `eq_kl_def`のいくつかのプロパティを見てみましょう。\n-  KL 発散は非対称です。つまり、 $$D_{\\mathrm{KL}}(P|Q) \\neq D_{\\mathrm{KL}}(Q|P).$ となる $P,Q$ が存在します。 $\n-  KL 発散は非負です。つまり $$D_{\\mathrm{KL}}(P|Q) \\geq 0.$$ 等式は $P = Q$ の場合にのみ成立することに注意してください。\n-  $p(x) &gt; 0$ かつ $q(x) = 0$ となる $x$ が存在する場合、$D_{\\mathrm{KL}}(P|Q) = \\infty$ となります。\n-  KL ダイバージェンスと相互情報量の間には密接な関係があります。 :numref: `fig_mutual_information`に示されている関係に加えて、 $I(X, Y)$ は次の項と数値的にも等価です。<ol><li> $D_{\\mathrm{KL}}(P(X, Y) \\ | \\ P(X)P(Y))$;\n-  $E_Y { D_{\\mathrm{KL}}(P(X \\mid Y) \\ | \\ P(X)) }$;\n-  $E_X { D_{\\mathrm{KL}}(P(Y \\mid X) \\ | \\ P(Y)) }$。\n</ol>最初の項では、相互情報量を $P(X, Y)$ と $P(X)$ と $P(Y)$ の積の間の KL 発散として解釈します。これは、ジョイントがどの程度異なるかを示す尺度です。独立している場合、配布は配布からのものです。第 2 項では、相互情報量により、$X$ の分布の値を学習した結果として生じる $Y$ についての不確実性の平均的な減少がわかります。 ３期も同様。</li>\n### 例\n\n非対称性を明示的に確認するためにおもちゃの例を見てみましょう。\n\nまず、長さ $10,000$ の 3 つのテンソルを生成してソートしましょう。正規分布 $N(0, 1)$ に従う目的テンソル $p$ と、正規分布 $N に従う 2 つの候補テンソル $q_1$ と $q_2$ です。それぞれ (-1, 1)$ と $N(1, 1)$ です。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ebd0b203",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "tensor_len = 10000\n",
        "p = torch.normal(0, 1, (tensor_len, ))\n",
        "q1 = torch.normal(-1, 1, (tensor_len, ))\n",
        "q2 = torch.normal(1, 1, (tensor_len, ))\n",
        "\n",
        "p = torch.sort(p)[0]\n",
        "q1 = torch.sort(q1)[0]\n",
        "q2 = torch.sort(q2)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e635f03",
      "metadata": {},
      "source": "\n$q_1$ と $q_2$ は y 軸に関して対称であるため (つまり、$x=0$)、$D_{\\mathrm{KL}}(p|q_1) の間の KL 発散の同様の値が期待されます。 $と$D_{\\mathrm{KL}}(p|q_2)$。以下に示すように、$D_{\\mathrm{KL}}(p|q_1)$ と $D_{\\mathrm{KL}}(p|q_2)$ の間の割引は 3% 未満のみです。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c56fa97c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8582.0341796875, 8828.3095703125, 2.8290698237936858)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kl_pq1 = kl_divergence(p, q1)\n",
        "kl_pq2 = kl_divergence(p, q2)\n",
        "similar_percentage = abs(kl_pq1 - kl_pq2) / ((kl_pq1 + kl_pq2) / 2) * 100\n",
        "\n",
        "kl_pq1, kl_pq2, similar_percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f11b947",
      "metadata": {},
      "source": "\n対照的に、$D_{\\mathrm{KL}}(q_2 |p)$ と $D_{\\mathrm{KL}}(p | q_2)$ は大きく外れており、図に示すように約 40% オフであることがわかります。下。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a8c06765",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(14130.125, 46.18621024399691)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kl_q2p = kl_divergence(q2, p)\n",
        "differ_percentage = abs(kl_q2p - kl_pq2) / ((kl_q2p + kl_pq2) / 2) * 100\n",
        "\n",
        "kl_q2p, differ_percentage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f76087",
      "metadata": {},
      "source": "\n## クロスエントロピー\n\n深層学習における情報理論の応用に興味がある場合は、ここに簡単な例を示します。真の分布 $P$ を確率分布 $p(x)$ で定義し、推定分布 $Q$ を確率分布 $q(x)$ で定義し、このセクションの残りの部分で使用します。\n\n与えられた $n$ データ例 {$x_1, \\ldots, x_n$} に基づいて二項分類問題を解く必要があるとします。 $1$ と $0$ をそれぞれ正と負のクラス ラベル $y_i$ としてエンコードし、ニューラル ネットワークが $\\theta$ によってパラメータ化されていると仮定します。 $\\hat{y} *i= p* {\\theta}(y_i \\mid x_i)$ となるような最良の $\\theta$ を見つけることを目指す場合、次のように最大対数尤度アプローチを適用するのが自然です。 numref: `sec_maximum_likelihood` 。具体的には、真のラベル $y_i$ と予測 $\\hat{y} *i= p* {\\theta}(y_i \\mid x_i)$ の場合、陽性として分類される確率は $\\pi_i= p_{\\theta} です。 (y_i = 1 \\mid x_i)$。したがって、対数尤度関数は次のようになります。\n\n $$ \\begin{aligned} l(\\theta) &amp;= \\log L(\\theta) \\ &amp;= \\log \\prod_{i=1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i} \\ &amp;= \\sum_{i=1}^n y_i \\log(\\pi_i) + (1 - y_i) \\log (1 - \\pi_i)。 \\ \\end{整列} $$\n\n対数尤度関数 $l(\\theta)$ を最大化することは $- l(\\theta)$ を最小化することと同じであるため、ここから最良の $\\theta$ を見つけることができます。上記の損失を任意の分布に一般化するために、$-l(\\theta)$ を*クロスエントロピー損失*$\\mathrm{CE}(y, \\hat{y})$ とも呼びます。ここで、$y$ は真の分布に従います。 $P$ と $\\hat{y}$ は推定分布 $Q$ に従います。\n\nこれはすべて、最尤性の観点から作業することによって導き出されました。しかし、よく見てみると、$\\log(\\pi_i)$ のような項が計算に含まれていることがわかります。これは、この式を情報理論の観点から理解できることを確実に示しています。\n\n### 正式な定義\n\nKL 発散と同様に、確率変数 $X$ について、*交差エントロピーを*介して推定分布 $Q$ と真の分布 $P$ の間の発散を測定することもできます。\n\n $$\\mathrm{CE}(P, Q) = - E_{x \\sim P} [\\log(q(x))].$$ :eqlabel: `eq_ce_def`\n\n上で議論したエントロピーの特性を使用することにより、それをエントロピー $H(P)$ と $P$ と $Q$ の間の KL 発散の合計として解釈することもできます。\n\n $$\\mathrm{CE} (P, Q) = H(P) + D_{\\mathrm{KL}}(P|Q).$$\n\nクロスエントロピー損失は次のように実装できます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e5942c8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "    ce = -torch.log(y_hat[range(len(y_hat)), y])\n",
        "    return ce.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795f23f6",
      "metadata": {},
      "source": "\n次に、ラベルと予測に対して 2 つのテンソルを定義し、それらのクロスエントロピー損失を計算します。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b6086d1a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9486)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = torch.tensor([0, 2])\n",
        "preds = torch.tensor([[0.3, 0.6, 0.1], [0.2, 0.3, 0.5]])\n",
        "\n",
        "cross_entropy(preds, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b29269",
      "metadata": {},
      "source": "\n### プロパティ\n\nこのセクションの冒頭で触れたように、cross-entropy :eqref: `eq_ce_def`を使用して、最適化問題の損失関数を定義できます。以下は同等であることがわかります。\n1. 分布 $P$ に対する $Q$ の予測確率を最大化します (つまり、 $E_{x \\sim P} [\\log (q(x))]$);\n1. クロスエントロピーの最小化 $\\mathrm{CE} (P, Q)$;\n1.  KL 発散 $D_{\\mathrm{KL}}(P|Q)$ を最小化します。\n\nクロスエントロピーの定義は、真のデータ $H(P)$ のエントロピーが一定である限り、目的 2 と目的 3 の間の等価関係を間接的に証明します。\n\n### 多クラス分類の目的関数としてのクロスエントロピー\n\nクロスエントロピー損失 $\\mathrm{CE}$ を伴う分類目的関数を深く掘り下げると、$\\mathrm{CE}$ を最小化することが対数尤度関数 $L$ を最大化することと同等であることがわかります。\n\nまず、$n$ 個の例を含むデータセットが与えられ、それを $k$ クラスに分類できるとします。各データ例 $i$ について、任意の $k$ クラス ラベル $\\mathbf{y} *i = (y* {i1}, \\ldots, y_{ik})$ を*ワンホット エンコーディング*で表します。具体的には、例 $i$ がクラス $j$ に属している場合、$j$ 番目のエントリを $1$ に設定し、他のすべてのコンポーネントを $0$ に設定します。\n\n $$ y_{ij} = \\begin{cases}1 &amp; j \\in J; \\ 0 &amp;\\text{そうでない場合。}\\end{ケース}$$\n\nたとえば、マルチクラス分類問題に 3 つのクラス $A$、$B$、$C$ が含まれている場合、ラベル $\\mathbf{y}_i$ は {$A: (1, 0, 0); B：（0、1、0）; C: (0, 0, 1)$}。\n\nニューラル ネットワークが $\\theta$ によってパラメータ化されていると仮定します。真のラベル ベクトル $\\mathbf{y *} i$ と予測 $$\\hat{\\mathbf{y}}_i= p* {\\theta}(\\mathbf{y} *i \\mid \\mathbf{x}_i) = \\ sum* {j=1}^k y_{ij} p_{\\theta} (y_{ij} \\mid \\mathbf{x}_i).$$\n\nしたがって、*クロスエントロピー損失は次*のようになります。\n\n $$ \\mathrm{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^n \\mathbf{y} *i \\log \\hat{\\mathbf{y }}_i = - \\sum* {i=1}^n \\sum_{j=1}^k y_{ij} \\log{p_{\\theta} (y_{ij} \\mid \\mathbf{x}_i)} .\\ $$\n\n一方で、最尤推定を通じて問題にアプローチすることもできます。まず、$k$ クラスのマルチヌーイ分布を簡単に紹介しましょう。これは、ベルヌーイ分布をバイナリ クラスからマルチクラスに拡張したものです。確率変数 $\\mathbf{z} = (z_{1}, \\ldots, z_{k})$ が確率 $\\mathbf{p} =$ ($p_{1) を持つ $k$ クラス*のマルチヌーリ分布*に従う場合}, \\ldots, p_{k}$)、つまり $$p(\\mathbf{z}) = p(z_1, \\ldots, z_k) = \\mathrm{Multi} (p_1, \\ldots, p_k), \\ text{ where } \\sum_{i=1}^k p_i = 1,$$ の場合、$\\mathbf{z}$ の同時確率質量関数 (pmf) は $$\\mathbf{p}^\\mathbf{z} となります。 = \\prod_{j=1}^k p_{j}^{z_{j}}.$$\n\n各データ例のラベル $\\mathbf{y} *i$ は、確率 $\\boldsymbol{\\pi} =$ ($\\pi* {1}, \\ldots、\\pi_{k}$)。したがって、各データ例 $\\mathbf{y *} i$ の結合 pmf は $\\mathbf{\\pi}^{\\mathbf{y}_i} = \\prod {j=1}^k \\pi_{j}^ となります。* {y_{ij}}.$ したがって、対数尤度関数は次のようになります。\n\n $$ \\begin{aligned} l(\\theta) = \\log L(\\theta) = \\log \\prod_{i=1}^n \\boldsymbol{\\pi}^{\\mathbf{y} *i} = \\log \\prod* {i=1}^n \\prod_{j=1}^k \\pi_{j}^{y_{ij}} = \\sum_{i=1}^n \\sum_{j=1}^k y_ {ij} \\log{\\pi_{j}}.\\ \\end{aligned} $$\n\n最尤推定では、 $\\pi_{j} = p_{\\theta} (y_{ij} \\mid \\mathbf{x}_i)$ とすることで目的関数 $l(\\theta)$ を最大化します。したがって、マルチクラス分類の場合、上記の対数尤度関数 $l(\\theta)$ を最大化することは、CE 損失 $\\mathrm{CE}(y, \\hat{y})$ を最小化することと同じです。\n\n上記の証明をテストするために、組み込みメジャー`NegativeLogLikelihood`を適用してみましょう。前の例と同じ`labels`と`preds`を使用すると、小数点第 5 位までは前の例と同じ数値損失が得られます。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2a1a528d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9486)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`\n",
        "# and `nn.NLLLoss()`\n",
        "nll_loss = NLLLoss()\n",
        "loss = nll_loss(torch.log(preds), labels)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d6b2f7",
      "metadata": {},
      "source": "\n## まとめ\n- 情報理論は、情報のエンコード、デコード、送信、操作に関する研究分野です。\n- エントロピーは、さまざまな信号にどれだけの情報が含まれているかを測定する単位です。\n-  KL ダイバージェンスは、2 つの分布間のダイバージェンスを測定することもできます。\n- クロスエントロピーは、マルチクラス分類の目的関数として見ることができます。クロスエントロピー損失を最小限に抑えることは、対数尤度関数を最大化することと同じです。\n\n## 演習\n1. 最初のセクションのカードの例に、要求されたエントロピーが実際に含まれていることを確認します。\n1.  KL 発散 $D(p|q)$ がすべての分布 $p$ と $q$ に対して非負であることを示します。ヒント: Jensen の不等式を使用します。つまり、$-\\log x$ が凸関数であるという事実を使用します。\n1. いくつかのデータ ソースからエントロピーを計算してみましょう。<ul><li>猿がタイプライターで生成した出力を見ていると仮定します。サルは、タイプライターの $44$ キーのいずれかをランダムに押します (特別なキーやシフト キーはまだ発見していないと想定できます)。 1 文字あたり何ビットのランダム性が観察されましたか?\n1. 猿に不満を抱いたあなたは、猿を酔った植字機に置き換えました。一貫性はありませんが、単語を生成することができます。代わりに、2,000 ドルの単語の語彙からランダムな単語を選択します。英語の単語の平均長が $4.5$ 文字であると仮定しましょう。現在、1 文字あたり何ビットのランダム性を観察していますか?\n1. それでも結果に満足できない場合は、タイプセッターを高品質の言語モデルに置き換えます。言語モデルは現在、単語ごとに $15$ ポイントという低い混乱度を取得できます。言語モデルの文字の*複雑さは*、一連の確率の幾何平均の逆数として定義され、各確率は単語内の文字に対応します。具体的には、指定された単語の長さが $l$ の場合、 $\\mathrm{PPL}(\\text{word}) = \\left[\\prod_i p(\\text{character}_i)\\right]^{ -\\frac{1}{l}} = \\exp \\left[ - \\frac{1}{l} \\sum_i{\\log p(\\text{character}_i)} \\right].$ テスト単語はは 4.5 文字ありますが、現在、1 文字あたり何ビットのランダム性が観察されていますか?\n</ul></li>1.  $I(X, Y) = H(X) - H(X \\mid Y)$ である理由を直感的に説明してください。次に、結合分布に関する期待値として両側を表現することによって、これが真実であることを示します。\n1.  2 つのガウス分布 $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ と $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ の間の KL 発散はいくらですか?\n"
    },
    {
      "cell_type": "markdown",
      "id": "52204985",
      "metadata": {},
      "source": "\n[ディスカッション](https://discuss.d2l.ai/t/1104)\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}