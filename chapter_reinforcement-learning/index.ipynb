{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c10dda2a",
      "metadata": {},
      "source": "\n# 強化学習\n\n:label: `chap_reinforcement_learning`\n\n **Pratik Chaudhari** (*ペンシルバニア大学および Amazon* )、 **Rasool Fakoor** ( *Amazon* )、および**Kavosh Asadi** ( *Amazon* )\n\n強化学習 (RL) は、意思決定を順番に行う機械学習システムの構築を可能にする一連の手法です。たとえば、オンライン小売業者から購入した新しい衣類が入った荷物は、小売業者があなたの家に最も近い倉庫から衣類を見つけ、その衣類を箱に入れ、その箱を運ぶなどの一連の決定を経て、あなたの玄関先に届きます。陸路または空路で市内のご自宅までお届けします。途中で荷物の配送に影響を与える変数は数多くあります。たとえば、衣類が倉庫にあるかどうか、箱の輸送にどれくらい時間がかかったか、毎日の配送トラックが出発する前にあなたの都市に到着したかどうか、重要な考え方は、各段階で、私たちがあまり制御できないこれらの変数が、将来の一連のイベント全体に影響を与えるということです。たとえば、倉庫での箱の梱包に遅れが生じた場合、小売業者は次の方法で荷物を発送する必要があるかもしれません。タイムリーな配送を確実にするために、地上ではなく空路で配送します。強化学習手法を使用すると、最終的に何らかの効用 (たとえば、荷物をタイムリーに届ける) を最大化するために、逐次的な意思決定問題の各段階で適切なアクションを実行できます。\n\nこのような一連の意思決定の問題は、他の多くの場所で見られます。たとえば、[囲碁](https://en.wikipedia.org/wiki/Go_(game)をプレイしているとき) 現在の手が次の手を決定し、相手の手が制御できない変数です...一連の動きによって、最終的に勝つかどうかが決まります。 ; Netflix があなたに勧める映画によって、あなたが何を観るかが決まります。あなたがその映画を好きかどうかは Netflix にはわかりません。最終的には、一連の映画のおすすめによって、あなたが Netflix にどの程度満足しているかが決まります。強化学習は現在、これらの問題に対する効果的な解決策を開発するために使用されています:cite: `mnih2013playing,silver2016mastering` 。強化学習と標準的な深層学習の主な違いは、標準的な深層学習では、1 つのテスト データに対するトレーニング済みモデルの予測が、将来のテスト データに対する予測に影響を与えないことです。強化学習では、将来の瞬間の決定 (RL では、決定はアクションとも呼ばれます) は、過去に行われた決定の影響を受けます。\n\nこの章では、強化学習の基礎を開発し、いくつかの一般的な強化学習手法を実装する実践的な経験を積みます。私たちはまず、このような逐次的意思決定問題を考えることを可能にするマルコフ決定プロセス (MDP) と呼ばれる概念を開発します。値反復と呼ばれるアルゴリズムは、MDP 内の非制御変数 (RL では、これらの制御変数は環境と呼ばれます) が通常どのように動作するかを知っているという前提の下で、強化学習の問題を解決するための最初の洞察になります。値の反復のより一般的なバージョンである Q ラーニングと呼ばれるアルゴリズムを使用すると、環境について必ずしも完全な知識を持っていない場合でも、適切なアクションを実行できるようになります。次に、専門家の行動を真似て、強化学習問題に対して深層ネットワークを使用する方法を研究します。そして最後に、ディープネットワークを利用して未知の環境で行動を起こす強化学習手法を開発します。これらの技術は、今日さまざまな現実世界のアプリケーションで使用されている、より高度な RL アルゴリズムの基礎を形成しており、その一部についてはこの章で説明します。\n\n![](../img/RL_main.png) :幅: `400px` :ラベル: `fig_rl_big`\n\n :begin_tab:toc\n-  [MDP](mdp.ipynb)\n- [値反復](value-iter.ipynb)\n- [qラーニング](qlearning.ipynb):end_tab:\n"
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}